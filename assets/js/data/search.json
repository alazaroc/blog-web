[ { "title": "Amazon GuardDuty: Deep dive", "url": "/posts/amazon-guardduty-deep-dive/", "categories": "Security", "tags": "security, guardduty, deep dive", "date": "2022-06-19 15:30:00 +0200", "snippet": "IntroductionAmazon GuardDuty is a continuous security monitoring service that analyzes and processes the following data sources: AWS CloudTrail management event logs, AWS CloudTrail data events for S3, DNS logs, EKS audit logs, and VPC flow logs.It uses threat intelligence feeds, such as lists of malicious IP addresses and domains, and machine learning to identify unexpected and potentially unauthorized and malicious activity within your AWS environment. Enabling this service is a MUST and a quick win to improve your security posture. With one-click Amazon GuardDuty reduces risk using intelligent and continuous threat detection of your AWS accounts, data, and workloads.Getting started GuardDuty is a Regional service, meaning any of the configuration procedures you follow must be repeated in each region that you want to monitor with GuardDuty.AWS highly recommend that you enable GuardDuty in all supported AWS Regions. This enables GuardDuty to generate findings about unauthorized or unusual activity even in Regions that you are not actively using.The first step to using GuardDuty is to enable it in your account. Once enabled, GuardDuty will immediately begin to monitor for security threats in the current region.Multi-account environmentWhen you use GuardDuty with AWS Organizations, you can designate any account within the organization to be the GuardDuty delegated administrator. Only the organization management account can designate GuardDuty delegated administrators.An account that is designated as a delegated administrator becomes a GuardDuty administrator account, has GuardDuty automatically enabled in the designated Region, and is granted permission to enable and manage GuardDuty for all accounts in the organization within that Region.More information.Standalone account environmentEnable GuardDuty is a one-click action:Estimating costsPricing (USD), Per account, per month, per Region AWS CloudTrail Management Event Analysis   Per one million events / month $4.00 per one million events AWS CloudTrail S3 Data Event Analysis   First 500 million events / month $0.80 per one million events Next 4,500 million events / month $0.40 per one million events Over 5,500 million events / month $0.20 per one million events Amazon EKS Audit Logs   There is no data in the official page There is no data in the official page VPC Flow Log and DNS Query Log Analysis   First 500 GB / month $1.00 per GB Next 2,000 GB / month $0.50 per GB Next 7,500 GB / month $0.25 per GB Over 10,000 GB / month $0.15 per GB You can use the GuardDuty console and API operations to estimate how much GuardDuty will cost you. During the 30-day free trial period, the cost estimation projects what your estimated costs will be after the free trial period.More information.Hands-on with Amazon GuardDutySample findingsGuardDuty supports generating sample findings with placeholder values, which can be used to test GuardDuty functionality and familiarize yourself with findings before needing to respond to a real security issue discovered by GuardDuty.These are the generated GuardDuty findings:Filtering findingsWe can filter the results by many criteria using the filter bar or we can filter using the colored buttons in the upper right section.You could also save your custom filters and using later.Suppression rulesIf you are receiving findings for expected behavior in your environment, you can automatically archive findings based on criteria you define with suppression rules. Suppression rules are rules which automatically send matched findings to archive.There are some cases where it is desirable to use suppression rules, to facilitate the recognition of security threats with the greatest impact on your environment: low-value findings false-positive findings threats you do not intend to act on Suppressed findings are not sent to AWS Security Hub, Amazon S3, Detective, or CloudWatch, reducing finding noise level if you consume GuardDuty findings via Security Hub, a third-party SIEM, or other alerting and ticketing applications.More information about suppression rules.Finding informationSelect a finding from the table to view its details. In the finding details pane you can see all the information associated with the finding:It contains the information related to the finding: overview section, resource affected, action, actor and additional information.Amazon GuardDuty integrates with Amazon Detective and if you click on the Investigate with Detective link you can view more information and the links to the Detective service to investigate it.More information about finding details.Custom responses with EventBridgeGuardDuty creates an event for EventBridge when any change in findings takes place (newly generated findings or newly aggregated findings).By using EventBridge events with GuardDuty, you can automate tasks to help you respond to security issues revealed by GuardDuty findings.More informationSubscribing to Amazon SNS GuardDuty announcementsAn Amazon SNS topic is created by GuardDuty (you cannot view it from the SNS console) and you can subscribe to receive notifications about newly released finding types, updates to the existing finding types, and other functionality changes.More information about how to do itRemediating security issuesThese are the recommended actions to remediate these scenarios.Remediating a compromised EC2 instanceThis is the information found in the official AWS documentation: Investigate the potentially compromised instance for malware and remove any discovered malware If you are unable to identify and stop unauthorized activity on your EC2 instance, AWS recommends that you terminate the compromised EC2 instance and replace it with a new instance as neededHowever, I think that this is more appropriate to deal with compromised EC2 instances: Lock the instance down Take the EBS snapshot Perform a memory dump Perform Forensic Analysis Terminate the instanceRemediating a compromised S3 Bucket Identify the affected S3 resource Identify the source of the suspicious activity and the API call used Determine whether the call source was authorized to access the identified resource. If the access was authorized, you can ignore the findingRemediating compromised AWS credentials Identify the affected IAM entity and the API call used Review permissions for the IAM entity Determine whether the IAM entity credentials were used legitimately. If you confirm that the activity is a legitimate use of the AWS credentials, you can ignore the GuardDuty findingRemediating Kubernetes security issues discovered by GuardDutyMore informationGood to know Regional service 30 days free trial when you enable it the first time, and during the free trial GuardDuty provides an estimate of what the spend would be, so you can assess your spending beyond the free trial. All findings are stored in GuardDuty for 90 days. GuardDuty recommends setting up findings export, which allows you to export your findings to an S3 bucket for indefinite storage. The events are delivered to EventBridge in near-real-time and on a best-effort basis. Amazon GuardDuty consumes CloudTrail management and S3 data events directly from CloudTrail through an independent and duplicative stream of events (no additional cost) Global events in CloudTrail (IAM, AWS Security Token Service, Amazon S3, Amazon CloudFront, and Route 53) are delivered to any trail that includes global services, and are logged as occurring in the US East (N. Virginia) Region. When you enable GuardDuty, it immediately starts analyzing your VPC flow logs data. It consumes VPC flow log events directly from the VPC flow logs feature through an independent and duplicative stream of flow logs, so Flow logs for VPCs do not need to be turned on to generate findings. All findings are dynamic, meaning that, if GuardDuty detects new activity related to the same security issue it will update the original finding with the new information, instead of generating a new finding Finding types (4): EC2, IAM, S3 and Kubernetes. The official documentation has a full explanation about each category and each finding inside, you can find it here. GuardDuty lists allow you to customize the publicly routable IP addresses that GuardDuty generates findings for. You can create a Trusted IP list and a Threat list.More information about GuardDuty Official documentation Amazon GuardDuty WorkshopComment this post I have temporarily added the comments section to the post here. In the future, I will add it in a better way and include all the validated comments (I guess that I will have to make a filter to avoid spam) Submit Submitting... Failed to submit the form Please, try again. Thank you! I will add your comment in a future. " }, { "title": "AWS Security Hub: Deep dive", "url": "/posts/aws-security-hub-deep-dive/", "categories": "Security", "tags": "security, security hub, deep dive", "date": "2022-06-17 23:20:00 +0200", "snippet": "IntroductionAWS Security Hub is a cloud security posture management service that: Reduced effort to collect and prioritize findings performs automatic security checks against best practices and standards aggregates your security alerts in a consolidated view of findings across accounts and providers enables automated remediationSecurity Hub enables you to understand your overall security posture via a consolidated security score across all of your AWS accounts, and automatically assesses the security of your AWS accounts resources via the security standards available: AWS Foundational Security Best Practices CIS AWS Foundations Benchmark PCI DSS Enabling this service is a MUST and a quick win to improve your security posture: just by enabling some security standard you will receive security alerts (findings), and the service will aggregate your security alerts automatically.How it worksTo maintain a complete view of your security posture on AWS, you need to integrate multiple tools and services including Threat detections from Amazon GuardDuty, vulnerabilities scan from Amazon Inspector, publicly accessible and cross-account resources from IAM Access Analyzer, sensitive data classifications from Amazon Macie, resources lacking WAF coverage from AWS Firewall Manager, resource configuration issues from AWS Config, and AWS Partner Network Products.More informationGetting startedWhether an account needs to enable AWS Security Hub manually depends on how the accounts are managed.Multi-account environmentIf you use the integration with AWS Organizations: The organization management account chooses a Security Hub administrator account. Security Hub is enabled automatically for the chosen account. See Designating a Security Hub administrator account. The Security Hub administrator account enables organization accounts as member accounts. Those organization accounts also have Security Hub enabled automatically. See Managing member accounts that belong to an organization. The only organization account for which Security Hub is not enabled automatically is the organization management account. The organization management account does not need to enable Security Hub before it designates the Security Hub administrator account. The organization management account must enable Security Hub before it is enabled as a member account.Standalone account environment Accounts that are not managed using the Organizations integration must enable Security Hub manually.To start using AWS Security Hub, it only takes a few clicks from the Management Console to start adding findings and performing security checks.As you can see in the image, you only need to check the Security Standard you want to enable and then enable de service. Additionally, you can download a CloudFormation template to deploy it as StackSet. After you enable a security standard, AWS Security Hub begins to run all checks within two hours but it can take up to 24 hours for the Security standards pane to populateEstimating costsPricing (USD), Per account, per month, per Region Security checks   First 100,000 $0.0010/check 100,001 – 500,000 $0.0008/check 500,001 + $0.0005/check Finding ingestion events   First 10,000 Free 10,001 + $0.00003/finding It includes ingestion of updates to existing findings. Finding ingestions for Security Hub security checks is free.Within the Security Hub service, under the Settings Usage tab, you can find your monthly estimate:Hands-on with AWS Security HubSummaryOn the Summary page: Security standards show you your security score (total and by standard), and the resources with the most failed security checks Findings by Region summarizes the number of active findings for each severity across Regions. The counts only include findings that have a workflow status of NEW or NOTIFIED. Insights: An AWS Security Hub insight is a collection of related findings. It identifies a security area that requires attention and intervention.Security StandardsThis page shows you the 3 security standards and whether you have them enabled (with their security score) or not (you can enable them with a one-click action).If you access one of them, you will be able to see more details about how many security controls are enabled, failed, passed, disabled… and a table with information about compliance status, severity, ID, title and Failed checks:FindingsThis page shows all the findings, the ones generated by AWS Security Hub with Security Standards and all the rest of the other tools and services.In the following image, you can view Security Hub and GuardDuty findings:Finding aggregation (cross-account &amp; cross-region)With finding aggregation, you can use a single aggregation account &amp; Region to view and update findings from multiple linked accounts &amp; Regions. Administrator accounts configure the aggregation. Security Hub replicates findings and finding updates for all of the member accounts in the linked Regions. You can connect multiple AWS accounts and consolidate findings across those accounts You can also designate an aggregator Region and link some or all Regions to that aggregator Region to give you a centralized view of all your findings across all your accounts and all your linked RegionsInteresting article in AWS Security Blog: Best practices for cross-Region aggregation of security findings. Best practices mentioned: Enable cross-Region aggregation Consolidating downstream SIEM and ticketing integrations Auto-archive GuardDuty findings associated with global resources Reduce AWS Config cost by recording global resources in one Region Disable AWS Security Hub AWS Foundational Best Practices periodic controls associated with global resources Implement automatic remediation from a central RegionYou also can show your AWS Security Hub findings in a view of data to enable decision-makers to assess the health and status of an organization’s IT infrastructure at a glance. This article contains how to do it.Disable controls When you enable a standard, all the controls for that standard are enabled by default. You can then disable and enable specific controls within an enabled standard.There are several reasons why you may choose to disable the controls: Controls for unused services Controls using global resources Controls with compensating controlsIt can be useful to turn off security checks for controls that are not relevant to your environment. Disabling irrelevant controls reduces the number of irrelevant findings. It also removes the failed check from the security score for the associated standard.When you disable a control, the following occurs: The check for the control is no longer performed. No additional findings are generated for that control. Existing findings are archived automatically after three to five days (note that this is the best effort and not guaranteed). The related AWS Config rules that Security Hub created are removed. Remember that Security Hub is Regional. When you disable or enable a control, it is disabled only in the current Region or in the Region that you specify in the API request.This is the fastest way to disable one control, with the CLI:aws securityhub update-standards-control --standards-control-arn \"arn:aws:securityhub:eu-west-1:1234567890:control/aws-foundational-security-best-practices/v/1.0.0/GuardDuty.1\" --control-status \"DISABLED\" --disabled-reason \"testing functionality\"With the AWS Console, you can easily disable a control. Access to a security standard:And then:In the following links, you will find more information on how to do it in-depth, how to do it in a multi-account environment (several options), and more relevant information! How to disable controls How to disable in a multi account environment AWS Foundational Best Practices controls that you might want to disable how to Create Auto-Suppression Rules in AWS Security HubIntegrationsSecurity Hub provides the ability to integrate security findings from AWS services and third-party products. For AWS services Security Hub automatically enables the integration, and you can optionally disable each integration. For third-party products Security Hub gives you the ability to selectively enable the integrations and provides a link to the configuration instructions related to the third-party product.Some examples about custom integrations: Integrate with Jira Service Management. Integrate with Slack. Summary email.Automated response, remediation, and enrichment actionsYou can create custom automated response, remediation, and enrichment workflows using Security Hub’s integration with Amazon EventBridge. All Security Hub findings are automatically sent to EventBridge, and you can create EventBridge rules that have AWS Lambda functions, AWS Step Function functions, or AWS Systems Manager Automation runbooks as their targets.Security Hub also supports sending findings to EventBridge on-demand via custom actions, so that you can have an analyst decide when to trigger an automated response or remediation action.The Security Hub Automated Response and Remediation (SHARR) solution provides you with pre-packaged EventBridge rules for you to deploy via AWS CloudFormation.More info about automated response and remediation hereGood to know Regional service 30 days free trial when you enable it the first time, and during the free trial Security Hub provides an estimate of what the spend would be, so you can assess your spending beyond the free trial. All findings are stored in Security Hub for 90 days after the last update date Periodic checks run automatically within 12 hours after the most recent run. You cannot change the periodicity. Change-triggered checks run when the associated resource changes state. Even if the resource does not change state, the updated time for change-triggered checks is refreshed every 18 hours. After control statuses are generated for the first time, Security Hub updates the control status every 24 hours based on the findings from the previous 24 hours Security Hub processes the findings using a standard findings format called the AWS Security Finding Format (ASFF), which eliminates the need for time-consuming data conversion efforts. With the ASFF, all of Security Hub’s integration partners (including both AWS services and external partners) send their findings to Security Hub in a well-typed JSON format consisting of over 1,000 available fields. This means that all of your security findings are normalized before they are ingested into Security Hub, and you don’t need to do any parsing and normalization yourself. The findings identify resources, severities, and timestamps consistently so that you can more easily search and take action on them. The events are delivered to EventBridge in near-real-time and on a guaranteed basis When you enabled a security standard, AWS Config rules are created automatically (and you cannot delete them) You can create Custom Findings with AWS Config Rules and send them to Security Hub with EventBridgeMore information about Security Hub AWS official documentation Security Hub Workshop Automatically resolve findings for resources that no longer exists Enrich findings with account metadata Correlate findingsComment this post I have temporarily added the comments section to the post here. In the future, I will add it in a better way and include all the validated comments (I guess that I will have to make a filter to avoid spam) Submit Submitting... Failed to submit the form Please, try again. Thank you! I will add your comment in a future. " }, { "title": "How to improve your account security", "url": "/posts/how-to-improve-your-account-security/", "categories": "Security", "tags": "security, getting started", "date": "2022-06-03 01:17:00 +0200", "snippet": "TLDRYou already have one or multiple AWS accounts and you want to improve your security approach, the Well-Architected Framework (security pillar) contains a lot of information, I did a full summary here, and you may want to learn about a plan to improve your account.I will share with you two resources to do it: AWS Security Maturity Model will allow you to know the recommended actions to strengthen your security posture at every stage of your journey to the cloud Contains 4 phases. The first one “quick wins” allow you fast security improvements Article in the AWS security blog: The top 10 most important cloud security tips that Stephen Schmidt, Chief Information Security Officer for AWS, laid out at AWS re:Invent 2019AWS Security Maturity ModelIt is a valuable resource for reviewing the current status and improving the security of your solutions.The classification of the different recommendations into the categories depends on the cost and difficulty of implementing the security control, and the positive impact that it will achieve. This model is updated monthly by AWS. In fact, a month after writing this article I had to update it because the model had changed and all areas of the organization had changed, although the content is the same.The official documentation is located here.IntroductionSecurity FrameworksMultiple frameworks help you design the construction of a plan to provide security to your loads in the cloud. Well Architected Framework NIST CyberSecurity Framework Center for Internet Security (CIS) AWS Foundations Cloud Adoption FrameworkHow to prioritize“With so many services, security controls, and recommendations… How do I prioritize? Where do I start?” All Quick Win recommendations can be implemented in less than a week.Evolutive pathPhase 1. Quick WinsQuick Wins are the first thing to focus on, controls that you could implement in an organization within a maximum of one or two weeks, and will significantly improve your security standpoint. Level Recommendation Security governance - Assign Security Contacts - Select the region(s) Security assurance - Automate alignment with best practices using AWS Security Hub Identity and Access management - Multi-Factor Authentication - Avoid using Root and audit it - Access and role analysis with IAM Access Analyzer Thread detection - Thread Detection with Amazon GuardDuty and review your findings - Audit API calls with AWS CloudTrail- Remediate security findings found by AWS Trusted Advisor - Billing alarms for anomaly detection Vulnerability management   Infrastructure protection - Limit Security Groups Data prevention - Amazon S3 Block Public Access - Analyze data security posture with Amazon Macie Application security - AWS WAF with managed rules Incident response - Act on Amazon GuardDuty findings Link to the updated content and more information on each recommendationPhase 2. FoundationalThe controls and recommendations may take some more effort to implement but are very important. Level Recommendation Security governance - Identify security and regulatory requirements - Identify the most sensitive data (crown jewels) - Cloud Security Training Plan Security assurance - Configuration monitoring with AWS Config Identity and Access management - Centralized user repository – Organization Policies (SCPs) Threat detection - Investigate most Amazon GuardDuty findings Vulnerability management - Manage vulnerabilities in your infrastructure and perform pentesting - Manage vulnerabilities in your application Infrastructure protection - Manage your instances with Fleet Manager - Network segmentation (Public/Private Networks - VPCs) - Multi-account management with AWS Control Tower Data protection - Data Encryption (AWS KMS) - Backups - Discover sensitive data with Amazon Macie Application security - Involve security teams in development - No secrets in your code AWS Secrets Manager Incident response - Define Incident response playbooks- Redundancy using multiple Availability Zones Link to the updated content and more information on each recommendationPhase 3. EfficientThere are some controls and recommendations that allow us to manage security in an efficient way. Level Recommendation Security governance - Perform thread modelling Security assurance - Create your reports for compliance (such as PCI-DSS) Identity and Access - Privilege review (Least Privilege) - Tagging strategy - Customer IAM: security of your customers Threat detection - Integration with SIEM/SOAR - Network Flows analysis (VPC Flow Logs) Vulnerability management - Security champion in development Infrastructure protection - Image Generation Pipeline - Anti-Malware/EDR - Outbound Traffic Control - Use abstract services (Serverless) Data protection - Encryption in transit Application security - WAF with custom rules - Shield Advanced Advanced DDoS Mitigation Incident response - Automate critical and most frequently run Playbooks - Automate deviation correction in configurations -Using infrastructure as Code (CloudFormation, CDK) Link to the updated content and more information on each recommendationPhase 4. OptimizedAnd finally, there are those controls and recommendations that allow you to optimize in a continuous improvement cycle the security posture every day. It will be characterized by security controls that are often seen in more mature organizations, in terms of security, or large organizations with very demanding requirements. Level Recommendation Security governance - Forming a Chaos Engineering team (Resilience) - Sharing Security work and responsibility Security assurance   Identity and Access management - Context-based access control - IAM Policy Generation Pipeline Threat detection - Amazon Fraud Detector - Integration with additional Intelligence Feeds Vulnerability management   Infrastructure protection - Process standardization with Service Catalog Data protection   Application security - DevSecOps - Forming a Red Team (Attacker’s Point of View) Incident response - Automate most playbooks - Amazon Detective: Root cause analysis - Forming a Blue Team (Incident Response) - Multi-region disaster recovery automation Link to the updated content and more information on each recommendationComplete Maturity LevelThis is the complete maturity model with all the phases through all the epics. I recommend you access to the original page to view the linkable table in the original site.Top 10 recommendations These are the top 10 most important cloud security tips that Stephen Schmidt, Chief Information Security Officer for AWS, laid out at AWS re:Invent 2019.The original article was written in the AWS blog here. Configure account contacts Use multi-factor authentication (MFA) is the best way to protect accounts from inappropriate access No hard-coding secrets use AWS Secrets Manager if you are using java or python you can use CodeGuru Reviewer to detect secrets in the code Limit security groups use AWS Config and AWS Firewall Manager to programmatically ensure that the virtual private cloud (VPC) security group configuration is what you intended Intentional data policies design your approach to data classification Centralize CloudTrail logs AWS recommends that you write logs in an AWS account designated for logging (Log Archive). The permissions on the bucket should prevent deletion of the logs, and they should also be encrypted at rest. Review how to use AWS to visualize AWS CloudTrail logs Validate IAM roles Use AWS IAM Access Analyzer Take action on findings Turn on AWS Security Hub, Amazon GuardDuty, and AWS Identity and Access Management Access Analyzer. You also need to take action when you see findings Rotate keys Be involved in the dev cycle “raise the security culture of your organization.” Comment this post I have temporarily added the comments section to the post here. In the future, I will add it in a better way and include all the validated comments (I guess that I will have to make a filter to avoid spam) Submit Submitting... Failed to submit the form Please, try again. Thank you! I will add your comment in a future. " }, { "title": "Getting Started with AWS Security", "url": "/posts/getting-started-with-aws-security/", "categories": "Security", "tags": "security, getting started", "date": "2022-05-31 19:33:00 +0200", "snippet": "TLDRYou have probably read many times that in AWS security is TOP priority, and as you know there are many resources on the internet. I want to share with you in this article the security basics to improve your AWS solutions by focusing on these 2 resources that you have to know: Recommendations and best practices: Security Pillar in AWS Well-Architected Framework AWS Security checklist If you’re looking to dive deeper into the broader range of learning materials available on security, including digital courses, blogs, whitepapers, and more, AWS recommends you the Ramp-Up Guide.Security Pillar in AWS Well-Architected FrameworkYou should start here. This is the official link. I am sure you are familiar with the Well-Architected Framework and the Security Pillar… but have you read the whole thing? I will try to compile the main points for you. The Security Pillar provides guidance to help you apply best practices, current recommendations in the design, delivery, and maintenance of secure AWS workloads. By adopting this practices you can build architectures that protect your data and systems, control access, and respond automatically to security events.Security in the cloud is composed of six areas: Foundations Identity and access management Detection (logging and monitoring) Infrastructure protection Data protection Incident response1. Security Foundations1.1. Design PrinciplesThe security pillar of the Well-Architected Framework captures a set of design principles that turn the security areas into practical guidance that can help you strengthen your workload security.Where the security epics frame the overall security strategy, these Well-Architected principles describe what you should start doing: Implement a strong identity foundation Implement the principle of least privilege Enforce separation of duties (with appropriate authorization) Centralize identity management Aim to eliminate reliance on long-term static credentials Enable traceability Monitor, alert, and audit actions and changes to your environment in real time Integrate log and metric collection with systems to automatically investigate and take action Apply security at all layers Apply a defense in depth approach with multiple security controls Apply to all layers (for example, edge of network, VPC, load balancing, every instance and compute service, operating system, application, and code) Automate security best practices Automated software-based security mechanisms (improve your ability to securely scale more rapidly and cost-effectively) Create secure architectures, including the implementation of controls that are defined and managed as code in version-controlled templates Protect data in transit and at rest Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate Keep people away from data Use mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data Prepare for security events Prepare for an incident by having incident management and investigation policy and processes that align to your organizational requirements Run incident response simulations and use tools with automation to increase your speed for detection, investigation, and recovery 1.2. Shared Responsibility Security and Compliance is a shared responsibility between AWS and the customer.More detailed information here.1.3. AWS Account Management and SeparationBest practices for account management and separation: Separate workloads using accounts Secure AWS account: not use the root user keep the contact information up to date Use AWS Organizations to: Manage accounts centrally: automates AWS account creation and management, and control of those accounts after they are created Set controls centrally: allows you to use service control policies (SCPs) to apply permission guardrails at the organization, organizational unit, or account level, which apply to all AWS Identity and Access Management (IAM) users and role Configure services and resources centrally: helps you configure AWS services that apply to all of your accounts (CloudTrail, AWS Config) 2. Identity and Access Management Identity and Access Management (IAM) helps customers integrate AWS into their identity management lifecycle, and sources of authentication and authorization.The best practices for these capabilities fall into two main areas. Identity Management Permissions Management2.1. Identity ManagementThere are two types of identities you will need to manage: Human identities: administrators, developers, operators, and consumers of your applications, … Machine identities: workload applications, operational tools, components, …The following are the best practices related to the identities: Rely on a centralized identity provider: This makes it easier to manage access across multiple applications and services, because you are creating, managing, and revoking access from a single location Federation with individual AWS Account: you can use centralized identities for AWS with a SAML 2.0-based provider with AWS IAM For federation to multiple accounts in your AWS Organization, you can configure your identity source in AWS Single Sign-On (AWS SSO) For managing end-users or consumers of your workloads, such as a mobile app, you can use Amazon Cognito Leverage user groups and attributes: Place users with common security requirements in groups defined by your identity provider, and put mechanisms in place to ensure that user attributes are correct and updated Use strong sign-in mechanisms Use temporary credentials Audit and rotate credentials periodically Store and use secrets securely: For credentials that are not IAM-related and cannot take advantage of temporary credentials, such as database logins, use a service that is designed to handle the management of secrets, such as AWS Secrets Manager2.2. Permission managementManage permissions to control access to human and machine identities that require access to AWS and your workloads. Permissions control who can access what, and under what conditions.How to grant access to different types of resources: Identity-based policies in IAM (managed or inline): These policies let you specify what that identity can do (its permissions) In most cases, you should create your own customer-managed policies following the principle of least privilege Resource-based policies are attached to a resource. These policies grant permission to a principal that can be in the same account as the resource or in another account Permissions boundaries: use a managed policy to set the maximum permissions that an administrator can set This enables you to delegate the ability to create and manage permissions to developers, such as the creation of an IAM role, but limit the permissions they can grant so that they cannot escalate their permission using what they have created Attribute-based access control (ABAC): enables you to grant permissions based on tags (attributes) Tags can be attached to IAM principals (users or roles) and to AWS resources Using IAM policies, administrators can create a reusable policy that applies permissions based on the attributes of the IAM principal Organizations service control policies (SCP): define the maximum permissions for account members of an organization or organizational unit (OU). Limit permission but do not grant it Session policies: advanced policies that you pass as a parameter when you programmatically create a temporary session for a role or federated user. These policies limit permissions but do not grant permissionsThe following are the best practices related to the permission management: Grant least privilege access Define permission guardrails for your organization: You should use AWS Organizations to establish common permission guardrails that restrict access to all identities in your organization. Here are examples of service control policies (SCPs) defined by AWS that you can apply to your organization. Analyze public and cross-account access: In AWS, you can grant access to resources in another account. You grant direct cross-account access using policies attached to resources or by allowing identity to assume an IAM role in another account. IAM Access Analyzer identify all access paths to a resource from outside of its account. It reviews resource policies continuously, and reports findings of public and cross-account access to make it easy for you to analyze potentially broad access. Share resources securely: AWS recommends sharing resources using AWS Resource Access Manager (AWS RAM) because enables you to easily and securely share AWS resources within your AWS Organization and Organizational Units Reduce permissions continuously: Maybe in the getting started of a project you chose to grant broad access, but later you should evaluate access continuously and restrict access to only the permissions required and achieve least privilege Establish emergency access process: AWS recommends having a process that allows emergency access to your workload, in particular your AWS accounts, in the unlikely event of an automated process or pipeline issue3. DetectionDetective Control provides guidance to help identify potential security incidents within the AWS environment. Detection consists of two parts: Configure Investigate3.1. Configure Configure services and application logging A foundational practice is to establish a set of detection mechanisms at the account level. This base set of mechanisms is aimed at recording and detecting a wide range of actions on all resources in your account. AWS CloudTrail provides event history of your AWS account activity AWS Config monitors and records your AWS resource configurations and allows you to automate the evaluation and remediation against desired configurations Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads AWS Security Hub provides a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services and optional third-party products to give you a comprehensive view of security alerts and compliance status. Many core AWS services provide service-level logging features. For example, Amazon VPC provides VPC Flow Logs Amazon CloudWatch Logs can be used to store and analyze logs for EC2 instances and application-based logging that doesn’t originate from AWS services (you will need an agent), and use CloudWatch Logs Insights to process them in real-time or dive into analysis. Analyze logs, findings, and metrics centrally: A best practice is to deeply integrate the flow of security events and findings into a notification and workflow system. GuardDuty and Security Hub provides aggregation, deduplication, and analysis mechanisms for log records that are also made available to you via other AWS services. 3.2. Investigate Implement actionable security events: For each detective mechanism you have, you should also have a process, in the form of a runbook or playbook, to investigate Automate response to events: In AWS, investigating events of interest and information on potentially unexpected changes into an automated workflow can be achieved using Amazon EventBridge Amazon GuardDuty also allows you to route events to a workflow system for those building incident response systems (Step Functions), to a central Security Account, or to a bucket for further analysis. Detecting change and routing this information to the correct workflow can also be accomplished using AWS Config Rules and Conformance Packs. Conformance packs are a collection of Config Rules and remediation actions you deploy as a single entity authored as a YAML template. A sample conformance pack template is available for the Well-Architected Security Pillar 4. Infrastructure Protection Infrastructure protection ensures that systems and resources within your workloads are protected against unintended and unauthorized access, and other potential vulnerabilities. You need to be familiar with Regions, Availability Zones, AWS Local Zones, and AWS Outposts.4.1. Protecting NetworksWhen you follow the principle of applying security at all layers, you employ a Zero Trust approach (application components don’t trust any other). Create network layers: Components that share reachability requirements can be segmented into layers formed by subnets. Control traffic at all layers: You should examine the connectivity requirements of each component. In a VPC (region level), the subnets are in an Availability Zone with Network ACLs and route tables associated, and inside of subnets, you include the use of security groups (stateful inspection firewall). Some AWS services require components to access the internet for making API calls, where AWS API endpoints are located. Other AWS services use VPC endpoints within your Amazon VPCs. Many AWS services, including Amazon S3 and Amazon DynamoDB, support VPC endpoints, and this technology has been generalized in AWS PrivateLink. AWS recommends you to use this approach to access AWS services, third-party services, and your own services hosted in other VPCs securely because all network traffic on AWS PrivateLink stays on the global AWS backbone and never traverses the internet. Connectivity can only be initiated by the consumer of the service, and not by the provider of the service. Implement inspection and protection: Inspect and filter your traffic at each layer. You can inspect your VPC configurations for potential unintended access using VPC Network Access Analyzer. For components transacting over HTTP-based protocols, a web application firewall, AWS WAF, can help protect from common attacks. AWS WAF lets you monitor and block HTTP(s) requests that match your configurable rules that are forwarded to an Amazon API Gateway API, Amazon CloudFront, or an Application Load Balancer. For managing AWS WAF, AWS Shield Advanced protection, and Amazon VPC security groups across AWS Organizations, you can use AWS Firewall Manager. It allows you to centrally configure and manage firewall rules across your accounts and applications, making it easier to scale enforcement of common rules. It also enables you to rapidly respond to attacks, using AWS Shield Advanced, or solutions that can automatically block unwanted requests to your web applications. Firewall Manager also works with AWS Network Firewall, a managed service that uses a rules engine to give you fine-grained control over both stateful and stateless network traffic. Automate network protection: Automate protection mechanisms to provide a self-defending network based on threat intelligence and anomaly detection. For example, intrusion detection and prevention tools can adapt to current threats and reduce their impact. A web application firewall is an example of where you can automate network protection, for example, by using the AWS WAF Security Automations solution (https://github.com/awslabs/aws-waf-security-automations ) to automatically block requests originating from IP addresses associated with known threat actors. 4.2. Protecting ComputeCompute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more. Each of these compute resource types requires different approaches to secure them. However, they do share common strategies that you need to consider: Perform vulnerability management: Frequently scan and patch for vulnerabilities in your code, dependencies, and in your infrastructure to help protect against new threats. Automate the creation of infrastructure with CloudFormation and create secure-by-default infrastructure templates verified with CloudFormation Guard For patch management, use AWS System Manager Patch Manager Reduce attack surface: Reduce your exposure to unintended access by hardening operating systems and minimizing the components, libraries, and externally consumable services in use. Reduce unused components In EC2 you can create your own AMIs simplifying the process with EC2 Image Builder. When using containers implement ECR Image Scanning Using third-party static code analysis tools, you can identify common security issues. You can use Amazon CodeGuru for supported languages. Dependency checking tools can also be used to determine whether libraries your code links against are the latest versions, are themselves free of CVEs, and have licensing conditions that meet your software policy requirements. Using Amazon Inspector, you can perform configuration assessments against your instances for known common vulnerabilities and exposures (CVEs), assess against security benchmarks and automate the notification of defects Enable people to perform actions at a distance: Removing the ability for interactive access reduces the risk of human error, and the potential for manual configuration or management. For example, use a change management workflow to manage EC2 instances using tools such as AWS Systems Manager instead of allowing direct access, or via a bastion host. AWS CloudFormation stacks build from pipelines and can automate your infrastructure deployment and management tasks without using the AWS Management Console or APIs directly. Implement managed services: Implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS, to reduce your security maintenance tasks as part of the shared responsibility model. This means you have more free time to focus on securing your application Validate software integrity: Implement mechanisms (e.g. code signing) to validate that the software, code, and libraries used in the workload are from trusted sources and have not been tampered with. You can use AWS Signer Automate compute protection: Automate your protective compute mechanisms including vulnerability management, reduction in attack surface, and management of resources. The automation will help you invest time in securing other aspects of your workload, and reduce the risk of human error.5. Data protection Before architecting any workload, foundational practices that influence security should be in place: data classification provides a way to categorize organizational data based on criticality and sensitivity in order to help you determine appropriate protection and retention controls encryption protects data by way of rendering it unintelligible to unauthorized access These methods are important because they support objectives such as preventing mishandling or complying with regulatory obligations.5.1. Data Classification Identify the data within your workload: You need to understand the type and classification of data your workload is processing, the associated business processes, data owner, applicable legal and compliance requirements, where it’s stored, and the resulting controls that are needed to be enforced. Define data protection controls: By using resource tags, separate AWS accounts per sensitivity, IAM policies, Organizations SCPs, AWS KMS, and AWS CloudHSM, you can define and implement your policies for data classification and protection with encryption. Define data lifecycle management: Your defined lifecycle strategy should be based on sensitivity level as well as legal and organizational requirements. Aspects including the duration for which you retain data, data destruction processes, data access management, data transformation, and data sharing should be considered. Automate identification and classification: Automating the identification and classification of data can help you implement the correct controls. Using automation for this instead of direct access from a person reduces the risk of human error and exposure. You should evaluate using a tool, such as Amazon Macie, that uses machine learning to automatically discover, classify, and protect sensitive data in AWS.5.2. Protecting data at restData at rest represents any data that you persist in non-volatile storage for any duration in your workload. This includes block storage, object storage, databases, archives, IoT devices, and any other storage medium on which data is persisted. Protecting your data at rest reduces the risk of unauthorized access when encryption and appropriate access controls are implemented.Encryption and tokenization are two important but distinct data protection schemes. Tokenization is a process that allows you to define a token to represent an otherwise sensitive piece of information. Encryption is a way of transforming content in a manner that makes it unreadable without a secret key necessary to decrypt the content back into plaintext.Best practices: Implement secure key management: By defining an encryption approach that includes the storage, rotation, and access control of keys, you can help provide protection for your content against unauthorized users and against unnecessary exposure to authorized users. AWS KMS helps you manage encryption keys and integrates with many AWS services. This service provides durable, secure, and redundant storage for your AWS KMS keys. AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys in the AWS Cloud. Enforce encryption at rest: You should ensure that the only way to store data is by using encryption. You can use AWS Managed Config Rules to check automatically that you are using encryption, for example, for EBS volumes, RDS instances, and S3 buckets. Enforce access control: Different controls including access (using least privilege), backups (see Reliability whitepaper), isolation, and versioning can all help protect your data at rest. Access to your data should be audited using detective mechanisms like CloudTrail and service level log You should inventory what data is publicly accessible, and plan for how you can reduce the amount of data available over time. Amazon S3 Glacier Vault Lock and S3 Object Lock are capabilities providing mandatory access control—once a vault policy is locked with the compliance option, not even the root user can change it until the lock expires Audit the use of encryption keys: Ensure that you understand and audit the use of encryption keys to validate that the access control mechanisms on the keys are appropriately implemented. For example, any AWS service using an AWS KMS key logs each use in AWS CloudTrail. You can then query AWS CloudTrail, by using a tool such as Amazon CloudWatch Insights, to ensure that all uses of your keys are valid. Use mechanisms to keep people away from data: Keep all users away from directly accessing sensitive data and systems under normal operational circumstances. For example, use a change management workflow to manage EC2 instances using tools instead of allowing direct access or a bastion host. This can be achieved using AWS Systems Manager Automation, which uses automation documents that contain steps you use to perform tasks. These documents can be stored in source control, be peer-reviewed before running, and tested thoroughly to minimize risk compared to shell access. Business users could have a dashboard instead of direct access to a data store to run queries. Where CI/CD pipelines are not used, determine which controls and processes are required to adequately provide a normally disabled break-glass access mechanism. Automate data at rest protection: Use automated tools to validate and enforce data at rest controls continuously, for example, verify that there are only encrypted storage resources. You can automate validation that all EBS volumes are encrypted using AWS Config Rules. AWS Security Hub can also verify a number of different controls through automated checks against security standards. Additionally, your AWS Config Rules can automatically remediate non-compliant resources. 5.3. Protecting data in transitData in transit is any data that is sent from one system to another. This includes communication between resources within your workload as well as communication between other services and your end-users. By providing the appropriate level of protection for your data in transit, you protect the confidentiality and integrity of your workload’s data.Best practices: Implement secure key and certificate management: Store encryption keys and certificates securely and rotate them at appropriate time intervals with strict access control. The best way to accomplish this is to use a managed service, such as AWS Certificate Manager (ACM). It lets you easily provision, manage, and deploy public and private Transport Layer Security (TLS) certificates for use with AWS services and your internal connected resources. Enforce encryption in transit: AWS services provides HTTPS endpoints using TLS for communication, thus providing encryption in transit when communicating with the AWS APIs. Insecure protocols, such as HTTP, can be audited and blocked in a VPC through the use of security groups. HTTP requests can also be automatically redirected to HTTPS in Amazon CloudFront or on an Application Load Balancer. Additionally, you can use VPN connectivity into your VPC from an external network to facilitate encryption of traffic. Third-party solutions are available in the AWS Marketplace if you have special requirements. Authenticate network communications: Using network protocols (TLS/IPsec) that support authentication allows for trust to be established between the parties adding encryption to reduce the risk of communications being altered or intercepted. Automate detection of unintended data access: Use tools such as Amazon GuardDuty automatically detects suspicious activity or attempts to move data outside of defined boundaries. Amazon VPC Flow Logs to capture network traffic information can be used with Amazon EventBridge to trigger the detection of abnormal connections–both successful and denied. S3 Access Analyzer can help assess what data is accessible to who in your S3 buckets. Secure data from between VPC or on-premises locations: You can use AWS PrivateLink to create a secure and private network connection between Amazon Virtual Private Cloud (Amazon VPC) or on-premises connectivity to services hosted in AWS. You can access AWS services, third-party services, and services in other AWS accounts as if they were on your private network. With AWS PrivateLink, you can access services across accounts with overlapping IP CIDRs without needing an Internet Gateway or NAT. You also do not have to configure firewall rules, path definitions, or route tables. Traffic stays on the Amazon backbone and doesn’t traverse the internet, therefore your data is protected. 6. Incident response Incident Response helps customers define and execute a response to security incidents.6.1. Design Goals of Cloud Response Establish response objectives: Some common goals include containing and mitigating the issue, recovering the affected resources, and preserving data for forensics, and attribution. Document plans: Create plans to help you respond to, communicate during, and recover from an incident. Respond using the cloud Know what you have and what you need: Preserve logs, snapshots, and other evidence by copying them to a centralized security cloud account. Use redeployment mechanisms: when possible, and make your response mechanisms safe to execute more than once and in environments in an unknown state. Automate where possible: As you see issues or incidents repeat and build mechanisms that programmatically triage and respond to common situations. Use human responses for unique, new, and sensitive incidents. Choose scalable solutions: reduce the time between detection and response. Learn and improve your process: When you identify gaps in your process, tools, or people, and implement plans to fix them. Simulations are safe methods to find gaps and improve processes.In AWS, there are several different approaches you can use when addressing incident response. Educate your security operations and incident response staff about cloud technologies and how your organization intends to use them. Development Skills: programming, source control, version control, CI/CD processes AWS Services: security services Application Awareness The best way to learn is hands-on, through running incident response game days Prepare your incident response team to detect and respond to incidents in the cloud, enable detective capabilities, and ensure appropriate access to the necessary tools and cloud services. Additionally, prepare the necessary runbooks, both manual and automated, to ensure reliable and consistent responses. Work with other teams to establish expected baseline operations, and use that knowledge to identify deviations from those normal operations. Simulate both expected and unexpected security events within your cloud environment to understand the effectiveness of your preparation. Iterate on the outcome of your simulation to improve the scale of your response posture, reduce time to value, and further reduce risk.AWS Security checklist This is a whitepaper of AWS that provides customer recommendations that align with the Well-Architected Framework Security Pillar. It is available here.1. Identity and Access Management Secure your AWS Account Use AWS Organizations Use the root user with MFA Configure account contacts Rely on centralized identity provider Centralize identities using either AWS Single Sign-On or a third-party provider to avoid routinely creating IAM users or using long-term access keys—this approach makes it easier to manage multiple AWS accounts and federated applications Use multiple AWS accounts Use of Service Control Policies to implement guardrails AWS Control Tower can help you easily set up and govern a multi-account AWS environment Store and use secrets securely Use AWS Secrets Manager if you cannot use temporary credentials 2. Detection Enable foundational services for all AWS accounts AWS CloudTrail to log API activity Amazon GuardDuty for continuous monitoring AWS Security Hub for a comprehensive view of your security posture Configure service and application-level logging In addition to your application logs, enable logging at the service level, such as Amazon VPC Flow Logs and Amazon S3, CloudTrail, and Elastic Load Balancer access logging, to gain visibility into events Configure logs to flow to a central account, and protect them from manipulation or deletion Configure monitoring and alerts, and investigate events Enable AWS Config to track the history of resources Enable Config Managed Rules to automatically alert or remediate undesired changes Configure alerts for all your sources of logs and events, from AWS CloudTrail to Amazon GuardDuty and your application logs, for high-priority events and investigate 3. Infrastructure protection Patch your operating system, applications, and code Use AWS Systems Manager Patch Manager to automate the patching process of all systems and code for which you are responsible, including your OS, applications, and code dependencies Implement distributed denial-of-service (DDoS) protection for your internet-facing resources Use Amazon Cloudfront, AWS WAF and AWS Shield to provide layer 7 and layer 3/layer 4 DDoS protection Control access using VPC Security Groups and subnet layers Use security groups for controlling inbound and outbound traffic, and automatically apply rules for both security groups and WAFs using AWS Firewall Manager Group different resources into different subnets to create routing layers, for example, database resources do not need a route to the internet 4. Data protection Protect data at rest Use AWS Key Management Service (KMS) to protect data at rest across a wide range of AWS services and your applications Enable default encryption for Amazon EBS volumes, and Amazon S3 buckets Encrypt data in transit Enable encryption for all network traffic, including Transport Layer Security (TLS) for web-based network infrastructure you control using AWS Certificate Manager to manage and provision certificates Use mechanisms to keep people away from data Keep all users away from directly accessing sensitive data and systems. For example, provide an Amazon QuickSight dashboard to business users instead of direct access to a database, and perform actions at a distance using AWS Systems Manager automation documents and Run Command 5. Incident response Ensure you have an incident response (IR) plan Begin your IR plan by building runbooks to respond to unexpected events in your workload Make sure that someone is notified to take action on critical findings Begin with GuardDuty findings. Turn on GuardDuty and ensure that someone with the ability to take action receives the notifications. Automatically creating trouble tickets is the best way to ensure that GuardDuty findings are integrated with your operational processes Practice responding to events Simulate and practice incident response by running regular game days, incorporating the lessons learned into your incident management plans, and continuously improving them Comment this post I have temporarily added the comments section to the post here. In the future, I will add it in a better way and include all the validated comments (I guess that I will have to make a filter to avoid spam) Submit Submitting... Failed to submit the form Please, try again. Thank you! I will add your comment in a future. " }, { "title": "How to create serverless applications with CDK and SAM", "url": "/posts/how-to-create-serverless-applications-with-cdk-and-sam/", "categories": "How-to, Serverless, IaC", "tags": "cdk, sam, comparative", "date": "2022-04-25 20:45:00 +0200", "snippet": "TLDR A serverless application is more than just a Lambda Function. It is a combination of Lambda functions, event sources, APIs, databases, and other resources that work together to perform tasks.Creating serverless resources from the AWS Console is quick and easy, but as you know you should only use it for testing purposes when you are learning how it works. After that, and thinking about how to use the resources in a real project, you will need: IaC: to create your resources in a way that allows you to recreate them easily Version control: to track all the code modifications CI/CD: to automate the release process Does anyone use CloudFormation or Terraform to manage their serverless resources? Possibly but come on, there is a better way! To manage your serverless resources, there are much better options: SAM (Serverless Application Model) / Serverless Framework: declarative option with templates. Specific frameworks for serverless applications CDK / Pulumi: add a level of abstraction and allow you to define the infrastructure with modern programming languages In this article, we will review the approach to combining both CDK + SAM. By the way, CDK + SAM is my preferred approach: you get the best of the 2 options!CDK vs SAMIn the following articles, you will find the basics of CDK and SAM. CDK basics: How to create infrastructure with CDK SAM basics: How to create serverless applications with SAMWhat do CDK and SAM have in common?Both… Are open-source, Apache-licensed software development frameworks Provide Infrastructure as Code (IaC) Use AWS CloudFormation behind the scenes to deploy resources Provide a CLI to build and deploy applications Are well integrated with AWS build pipelines Support component reuseWhat are the main differences between CDK and SAM?   CDK SAM To declare resources Uses familiar programming languages Uses JSON or YAML Dynamic references Native language capabilities Pseudo parameters and logical functions Testing Not supported natively (you could use SAM) Supported (also debug) IaC resources All Focus on serverless Complexity Very low High, verbose configuration Maintainability Higher Lower Demo: CDK + SAMFrom Jan 6, 2022, AWS announced the general availability of AWS SAM CLI support for local testing of AWS CDK applications. It means that you can use SAM over your CDK project to test your resources!So… we will use a new CDK project to show the CDK + SAM.The source code is available here. This repository has several CDK projects but first, we will use the v1-simplePrepare to testWith CDK, when you run cdk synth, it will synthesize a stack defined in your app into a CloudFormation template in a json file in the cdk.out folder.However, SAM uses a yaml template, template.yaml or template.yml, in the root folder. Also, to test locally, you will need this file created or you will receive an error &gt; sam local invokeError: Template file not found at /.../aws-cdk-simple-webservice/template.yml Then, we have to run cdk synth and store the result in template.yml file.cdk synth --no-staging &gt; template.yml You have to use --no-staging because it is required for SAM CLI to local debug the source files.Testing Lambda FunctionsYou have now a template.yml file and can run the SAM command to test your lambda function.&gt; sam local invokeInvoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v1-simple/functions/simplest-example as /var/task:ro,delegated inside runtime containerSTART RequestId: 03d4ef7d-47b4-4ad2-a491-d0e5fc797ece Version: $LATEST2022-04-22T21:00:38.952Z 03d4ef7d-47b4-4ad2-a491-d0e5fc797ece INFO request: {}END RequestId: 03d4ef7d-47b4-4ad2-a491-d0e5fc797eceREPORT RequestId: 03d4ef7d-47b4-4ad2-a491-d0e5fc797ece Init Duration: 0.39 ms Duration: 205.98 ms Billed Duration: 206 ms Memory Size: 512 MB Max Memory Used: 512 MB{\"statusCode\":200,\"headers\":{\"Content-Type\":\"text/html\"},\"body\":\"You have connected with the Lambda!\"}%The Lambda returns the following body: You have connected with the Lambda!Testing Lambda Functions with input dataIf your Lambda Functions need input data, you can generate it from SAM CLI with the command generate-eventsam local generate-event [OPTIONS] COMMAND [ARGS]...You can use this command to generate sample payloads from different event sources such as S3, API Gateway, SNS, and so on. These payloads contain the information that the event sources send to your Lambda functions.Or you can add the input data to the option -e of the command invoke&gt; sam local invoke -e test/events/simple-event.jsonInvoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v1-simple/functions/simplest-example as /var/task:ro,delegated inside runtime container} \"rawPath\": \"/test\"706Z 992499c1-83c4-408d-966b-2e13f5955cbc INFO input: {{\"statusCode\":200,\"headers\":{\"Content-Type\":\"text/html\"},\"body\":\"You have connected with the Lambda!\"}END RequestId: 992499c1-83c4-408d-966b-2e13f5955cbcREPORT RequestId: 992499c1-83c4-408d-966b-2e13f5955cbc Init Duration: 0.89 ms Duration: 244.32 ms Billed Duration: 245 ms Memory Size: 512 MB Max Memory Used: 512 MBTesting API GatewayYou have to run sam local start-api&gt; sam local start-apiMounting simplest-lambda at http://127.0.0.1:3000$default [X-AMAZON-APIGATEWAY-ANY-METHOD]You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template2022-04-23 00:03:58 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit) You can access http://127.0.0.1:3000/ to connect with your Lambda FunctionIf you review your previous console, it will be updated when you accessed your API Gateway:&gt; sam local start-apidefault [X-AMAZON-APIGATEWAY-ANY-METHOD]You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template2022-04-23 00:03:58 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit)Invoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v1-simple/functions/simplest-example as /var/task:ro,delegated inside runtime containerSTART RequestId: 8ed4a0a8-18bc-43af-b759-ad0668784351 Version: $LATEST \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Ge \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng} \"isBase64Encoded\": falsehost\"01 +0000\",-11ba1c012bf1\",END RequestId: 8ed4a0a8-18bc-43af-b759-ad0668784351REPORT RequestId: 8ed4a0a8-18bc-43af-b759-ad0668784351 Init Duration: 0.51 ms Duration: 230.55 ms Billed Duration: 231 ms Memory Size: 512 MB Max Memory Used: 512 MB2022-04-23 00:11:05 127.0.0.1 - - [23/Apr/2022 00:11:05] \"GET / HTTP/1.1\" 200 -Invoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v1-simple/functions/simplest-example as /var/task:ro,delegated inside runtime containerSTART RequestId: 5759a74a-40b5-4a7e-8362-eec719ae44a7 Version: $LATEST \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Ge} \"isBase64Encoded\": falseco\"t\"01 +0000\",-11ba1c012bf1\",g+xml,image/*,*/*;q=0.8\",END RequestId: 5759a74a-40b5-4a7e-8362-eec719ae44a7REPORT RequestId: 5759a74a-40b5-4a7e-8362-eec719ae44a7 Init Duration: 0.50 ms Duration: 238.45 ms Billed Duration: 239 ms Memory Size: 512 MB Max Memory Used: 512 MB2022-04-23 00:11:06 127.0.0.1 - - [23/Apr/2022 00:11:06] \"GET /favicon.ico HTTP/1.1\" 200 -Bonus: Testing DynamoDBOk, testing a mocked Lambda Function is the “hello world” example and not very useful, but what about a Lambda Function that connects to a DynamoDB table?We will update our Lambda Function to store the data in a DynamoDB table, so we are using the v2-dynamodb example in the repository. This code is based on the pattern defined in the web cdkpatterns as the simple webservice.Case 1: Testing cloud DynamoDBWhen you try to locally test a Lambda Function that stores data in a DynamoDB table, it will automatically attempt to connect to the DynamoDB service of your AWS Account.So, to test your Account DynamoDB tables, you have to do nothing.&gt; sam local invoke -e test/events/simple-event.jsonInvoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v2-dynamodb/functions/dynamodb-example as /var/task:ro,delegated inside runtime container} \"rawPath\": \"/test\"492Z 69d093d2-083c-46e7-a318-636ed94d7e47 INFO request: {2022-04-23T06:38:56.797Z 69d093d2-083c-46e7-a318-636ed94d7e47 ERROR Invoke Error {\"errorType\":\"ResourceNotFoundException\",\"errorMessage\":\"Requested resource not found\",\"code\":\"ResourceNotFoundException\",\"message\":\"Requested resource not found\",\"time\":\"2022-04-23T06:38:56.787Z\",\"requestId\":\"RCJDUN8DRCPPOS3SR3034ETK8VVV4KQNSO5AEMVJF66Q9ASUAAJG\",\"statusCode\":400,\"retryable\":false,\"retryDelay\":49.42319019990148,\"stack\":[\"ResourceNotFoundException: Requested resource not found\",\" at Request.extractError (/var/runtime/node_modules/aws-sdk/lib/protocol/json.js:52:27)\",\" at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:106:20)\",\" at Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:78:10)\",\" at Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:686:14)\",\" at Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)\",\" at AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)\",\" at /var/runtime/node_modules/aws-sdk/lib/state_machine.js:26:10\",\" at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:38:9)\",\" at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:688:12)\",\" at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:116:18)\"]}{\"errorType\":\"ResourceNotFoundException\",\"errorMessage\":\"Requested resource not found\",\"trace\":[\"ResourceNotFoundException: Requested resource not found\",\" at Request.extractError (/var/runtime/node_modules/aws-sdk/lib/protocol/json.js:52:27)\",\" at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:106:20)\",\" at Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:78:10)\",\" at Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:686:14)\",\" at Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)\",\" at AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)\",\" at /var/runtime/node_modules/aws-sdk/lib/state_machine.js:26:10\",\" at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:38:9)\",\" at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:688:12)\",\" at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequentialEND RequestId: 69d093d2-083c-46e7-a318-636ed94d7e47REPORT RequestId: 69d093d2-083c-46e7-a318-636ed94d7e47 Init Duration: 0.98 ms Duration: 928.39 ms Billed Duration: 929 ms Memory Size: 512 MB Max Memory Used: 512 MB_executor.js:116:18)\"]}% If you don’t deploy your CDK project before attempting to test it, you will get the following ERROR: \"errorType\":\"ResourceNotFoundException\",\"errorMessage\":\"Requested resource not found\"Of course, you can set your AWS account in your sam CLI using the profile command. If you do not specify it, the default value will be applied.&gt; sam local invoke -e test/events/simple-event.json profile testInvoking index.handler (nodejs14.x)...Case 2: Testing local DynamoDBYou may want to test your Lambda function locally instead of connecting to your DynamoDB account, so do the following: Download DynamoDB docker image Run the DynamoDB docker image Set up DynamoDB: create tables, insert data, test it Change your Lambda Function codeDownload the DynamoDB docker imageThe first step is to download the DynamoDB Docker image.&gt; docker pull amazon/dynamodb-localUsing default tag: latestlatest: Pulling from amazon/dynamodb-local3a461b3ae562: Pull complete14d349bd5978: Pull complete3e361eec6409: Pull completeDigest: sha256:07e740ad576acdcfdc48676f9a153a93a8e35436ea36942d4c14939caeca8851Status: Downloaded newer image for amazon/dynamodb-local:latestdocker.io/amazon/dynamodb-local:latestRun the DynamoDB Docker image locallyNow we have to run the locally downloaded docker image. This terminal tab will be kept running and you will have to open another one.&gt; docker run -p 8000:8000 amazon/dynamodb-localInitializing DynamoDB Local with the following configuration:Port: 8000InMemory: trueDbPath: nullSharedDb: falseshouldDelayTransientStatuses: falseCorsParams: * This command will not persist data in the local DynamoDB.Create a local DynamoDB tableWe are going to create a table with the name hits, with a partitionKey with the name path and the String type.&gt; aws dynamodb create-table --table-name hits --attribute-definitions AttributeName=path,AttributeType=S --key-schema AttributeName=path,KeyType=HASH --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1 --endpoint-url http://localhost:8000{ \"TableDescription\": { \"TableArn\": \"arn:aws:dynamodb:ddblocal:000000000000:table/hits\", \"AttributeDefinitions\": [ { \"AttributeName\": \"path\", \"AttributeType\": \"S\" } ], \"ProvisionedThroughput\": { \"NumberOfDecreasesToday\": 0, \"WriteCapacityUnits\": 1, \"LastIncreaseDateTime\": 0.0, \"ReadCapacityUnits\": 1, \"LastDecreaseDateTime\": 0.0 }, \"TableSizeBytes\": 0, \"TableName\": \"hits\", \"TableStatus\": \"ACTIVE\", \"KeySchema\": [ { \"KeyType\": \"HASH\", \"AttributeName\": \"path\" } ], \"ItemCount\": 0, \"CreationDateTime\": 1650668228.617 }}Add values to our local DynamoDB tableWe will add 2 elements: path: “/test” path: “/hello”aws dynamodb put-item --table-name hits --item '{ \"path\": {\"S\": \"/test\"} }' --return-consumed-capacity TOTAL --endpoint-url http://localhost:8000aws dynamodb put-item --table-name hits --item '{ \"path\": {\"S\": \"/hello\"} }' --return-consumed-capacity TOTAL --endpoint-url http://localhost:8000Scan your table locallyWe check that our table has the created elements:&gt; aws dynamodb scan --table-name hits --endpoint-url http://localhost:8000{ \"Count\": 2, \"Items\": [ { \"path\": { \"S\": \"/test\" } }, { \"path\": { \"S\": \"/hello\" } } ], \"ScannedCount\": 2, \"ConsumedCapacity\": null}Change Lambda Function codeWe need to update our Lambda Function code to tell DynamoDB to read from our local DynamoDB service: You have to use your specific docker endpointif (process.env.AWS_SAM_LOCAL) { // mac dynamo.endpoint = new AWS.Endpoint(\"http://docker.for.mac.localhost:8000/\"); // windows // dynamo.endpoint = new AWS.Endpoint(\"http://docker.for.windows.localhost:8000/\"); // linux // dynamo.endpoint = new AWS.Endpoint(\"http://127.0.0.1:8000\");}Test DynamoDB locallyIn summary, we have: the DynamoDB service locally running a table (hits) 2 elements path=/test path=/hello Now we are going to test our Lambda Function which will insert data into our local DynamoDB table.&gt; sam local invoke -e test/events/simple-event.jsonInvoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v2-dynamodb/functions/dynamodb-example as /var/task:ro,delegated inside runtime container} \"rawPath\": \"/test\"036Z eaf85e61-e9a2-4b49-9953-d247f9794fb8 INFO request: {2022-04-22T23:54:20.130Z eaf85e61-e9a2-4b49-9953-d247f9794fb8 INFO inserted counter for /testEND RequestId: eaf85e61-e9a2-4b49-9953-d247f9794fb8REPORT RequestId: eaf85e61-e9a2-4b49-9953-d247f9794fb8 Init Duration: 0.48 ms Duration: 697.40 ms Billed Duration: 698 ms Memory Size: 512 MB Max Memory Used: 512 MB{\"statusCode\":200,\"headers\":{\"Content-Type\":\"text/html\"},\"body\":\"You have connected with the Lambda and store the data in the DynamoDB table!\"}If we scan the table again, we can review that in “/test” element will be a new hits column and 2 values:&gt; aws dynamodb scan --table-name hits --endpoint-url http://localhost:8000{ \"Count\": 2, \"Items\": [ { \"path\": { \"S\": \"/test\" }, \"hits\": { \"N\": \"2\" } }, { \"path\": { \"S\": \"/hello\" } } ], \"ScannedCount\": 2, \"ConsumedCapacity\": null}If you run your function more times, the value of hits will be updated.And, of course, you can also test it from API Gateway:&gt; sam local start-apiMounting dynamodb-lambda at http://127.0.0.1:3000$default [X-AMAZON-APIGATEWAY-ANY-METHOD]You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template2022-04-25 19:53:01 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit)Comment this post I have temporarily added the comments section to the post here. In the future, I will add it in a better way and include all the validated comments (I guess that I will have to make a filter to avoid spam) Submit Submitting... Failed to submit the form Please, try again. Thank you! I will add your comment in a future. " }, { "title": "How to add CI/CD to my SAM project", "url": "/posts/how-to-add-ci-cd-to-my-sam-project/", "categories": "How-to, Serverless, IaC", "tags": "sam, cicd, codepipeline, codebuild, github", "date": "2022-04-10 10:53:00 +0200", "snippet": "Introduction This is the second article of SAM. In this other article I had explained how to create serverless applications with SAM, and there I explained all the basic SAM information, so you may need to review it before this.Here we will add CI/CD to our SAM application through the pipeline integration of the AWS SAM CLI. As we want to add automation to our deployment process and integrate it with the AWS ecosystem, we will use the AWS Developer tools.This is the SAM project code on GitHub that we will use in the article. In the commit history you can find the evolution of the application through the steps explained.Add CI/CD to a SAM projectWe want to create a CI/CD pipeline to implement continuous deployment (we want that when we push new code, the pipeline deploys our resources automatically). From AWS doc: AWS SAM provides a set of default pipeline templates for multiple CI/CD systems that encapsulate AWS’s deployment best practices. These default pipeline templates use standard JSON/YAML pipeline configuration formats, and the built-in best practices help perform multi-account and multi-region deployments and verify that pipelines cannot make unintended changes to infrastructure.We want to create a new pipeline in the AWS CodePipeline resource using the SAM templates.To generate a starter pipeline configuration for AWS CodePipeline, we have to perform the following tasks in this order: Create infrastructure resources Generate the pipeline configuration Commit your pipeline configuration to the Git repository Deploy your pipeline Connect your Git repository with your CI/CD system After you’ve generated the starter pipeline configuration and committed it to your Git repository, whenever someone commits a code change to that repository your pipeline will be triggered to run automatically.Step 1: Create infrastructure resourcesPipelines that use AWS SAM require certain AWS resources, like an IAM user and roles with necessary permissions, an Amazon S3 bucket, and optionally an Amazon ECR repository. You must have a set of infrastructure resources for each deployment stage of the pipeline.For each stage we need (dev, test, prod…), we have to run sam pipeline bootstrap, and it will create a CloudFormation stack with the name aws-sam-cli-managed-${stage}-pipeline-resources, which will create the necessary resources that SAM needs.We are going to create only one stage with name test:&gt; sam pipeline bootstrapsam pipeline bootstrap generates the required AWS infrastructure resources to connectto your CI/CD system. This step must be run for each deployment stage in your pipeline,prior to running the sam pipeline init command.We will ask for [1] stage definition, [2] account details, and[3] references to existing resources in order to bootstrap these pipeline resources.[1] Stage definitionEnter a configuration name for this stage. This will be referenced later when you use the sam pipeline init command:Stage configuration name: &gt; test[2] Account detailsThe following AWS credential sources are available to use.To know more about configuration AWS credentials, visit the link below:&lt;https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html&gt;1 - Environment variables (not available)2 - default (named profile)3 - localstack (named profile)q - Quit and configure AWS credentialsSelect a credential source to associate with this stage: &gt; 2Associated account xxxxxxxxxxxx with configuration test.Enter the region in which you want these resources to be created [eu-west-1]:Enter the pipeline IAM user ARN if you have previously created one, or we will create one for you []:[3] Reference application build resourcesEnter the pipeline execution role ARN if you have previously created one, or we will create one for you []: &gt;Enter the CloudFormation execution role ARN if you have previously created one, or we will create one for you []: &gt;Please enter the artifact bucket ARN for your Lambda function. If you do not have a bucket, we will create one for you []: &gt;Does your application contain any IMAGE type Lambda functions? [y/N]: &gt;[4] SummaryBelow is the summary of the answers:1 - Account: xxxxxxxxxxxx2 - Stage configuration name: test3 - Region: eu-west-14 - Pipeline user: [to be created]5 - Pipeline execution role: [to be created]6 - CloudFormation execution role: [to be created]7 - Artifacts bucket: [to be created]8 - ECR image repository: [skipped]Press enter to confirm the values above, or select an item to edit the value: &gt;This will create the following required resources for the 'test' configuration:- Pipeline IAM user- Pipeline execution role- CloudFormation execution role- Artifact bucketShould we proceed with the creation? [y/N]: &gt; YCreating the required resources...Successfully created!The following resources were created in your account:- Pipeline IAM user- Pipeline execution role- CloudFormation execution role- Artifact bucketPipeline IAM user credential:AWS_ACCESS_KEY_ID: xxxxxxxxxxAWS_SECRET_ACCESS_KEY: xxxxxxxxxxView the definition in .aws-sam/pipeline/pipelineconfig.toml,run sam pipeline bootstrap to generate another set of resources, or proceed tosam pipeline init to create your pipeline configuration file.Before running sam pipeline init, we recommend first setting up AWS credentialsin your CI/CD account. Read more about how to do so with your provider in&lt;https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-generating-example-ci-cd-others.html&gt;.A new stack is created for our new stage test. If you want more than 1 stage you should repeat the sam pipeline bootstrap for the new stage (prod?). In this example I only want 1 stage to simplify.In our SAM project now we have 1 new file containing our stage information:Step 2: Generate the pipeline configurationTo generate the pipeline configuration, run the command sam pipeline init:&gt; sam pipeline initsam pipeline init generates a pipeline configuration file that your CI/CD systemcan use to deploy serverless applications using AWS SAM.We will guide you through the process to bootstrap resources for each stage,then walk through the details necessary for creating the pipeline config file.Please ensure you are in the root folder of your SAM application before you begin.Select a pipeline template to get started: 1 - AWS Quick Start Pipeline Templates 2 - Custom Pipeline Template LocationChoice: &gt; 1Cloning from https://github.com/aws/aws-sam-cli-pipeline-init-templates.git (process may take a moment)Select CI/CD system 1 - Jenkins 2 - GitLab CI/CD 3 - GitHub Actions 4 - Bitbucket Pipelines 5 - AWS CodePipelineChoice: &gt; 5You are using the 2-stage pipeline template. _________ _________| | | || Stage 1 |-&gt;| Stage 2 ||_________| |_________|Checking for existing stages...Only 1 stage(s) were detected, fewer than what the template requires: 2.To set up stage(s), please quit the process using Ctrl+C and use one of the following commands:sam pipeline init --bootstrap To be guided through the stage and config file creation process.sam pipeline bootstrap To specify details for an individual stage.To reference stage resources bootstrapped in a different account, press enter to proceed []:What is the Git provider? 1 - Bitbucket 2 - CodeCommit 3 - GitHub 4 - GitHubEnterpriseServerChoice []: &gt; 3What is the full repository id (Example: some-user/my-repo)?: alazaroc/aws-sam-appWhat is the Git branch used for production deployments? [main]:What is the template file path? [template.yaml]:We use the stage configuration name to automatically retrieve the bootstrapped resources created when you ran `sam pipeline bootstrap`.Here are the stage configuration names detected in .aws-sam/pipeline/pipelineconfig.toml: 1 - testSelect an index or enter the stage 1's configuration name (as provided during the bootstrapping): &gt; 1What is the sam application stack name for stage 1? [sam-app]: &gt;Stage 1 configured successfully, configuring stage 2.Here are the stage configuration names detected in .aws-sam/pipeline/pipelineconfig.toml: 1 - testSelect an index or enter the stage 2's configuration name (as provided during the bootstrapping):I have to stop the execution here.In the console log below, the following is displayed: You are using the 2-stage pipeline template Only 1 stage(s) were detected, fewer than what the template requires: 2. In any real project you should have at least two stages, so you could use the default AWS template.I don't want to create two stages in my CI/CD pipeline, I am testing a simple SAM project, and I only want ONE. Unfortunately, you can’t do that with the AWS Quick Start Pipeline Templates so I forked the main AWS project and I created a custom template with only ONE stage. This is my forked project: https://github.com/alazaroc/aws-sam-cli-pipeline-init-templates.git. I had to put my custom template in the root folder because otherwise, the AWS SAM CLI doesn’t work.In the next execution, I will choose option 2 Custom Pipeline Template Location, and add it to my updated forked repository to create only one stage in the CodePipeline.&gt; sam pipeline initsam pipeline init generates a pipeline configuration file that your CI/CD systemcan use to deploy serverless applications using AWS SAM.We will guide you through the process to bootstrap resources for each stage,then walk through the details necessary for creating the pipeline config file.Please ensure you are in the root folder of your SAM application before you begin.Select a pipeline template to get started: 1 - AWS Quick Start Pipeline Templates 2 - Custom Pipeline Template LocationChoice: &gt; 2Template Git location: &gt; https://github.com/alazaroc/aws-sam-cli-pipeline-init-templates.gitCloning from https://github.com/alazaroc/aws-sam-cli-pipeline-init-templates.git (process may take a moment)You are using the 1-stage pipeline template. _________| || Stage 1 ||_________|Checking for existing stages...What is the Git provider? 1 - Bitbucket 2 - CodeCommit 3 - GitHub 4 - GitHubEnterpriseServerChoice []: &gt; 3What is the full repository id (Example: some-user/my-repo)?: &gt; alazaroc/aws-sam-appWhat is the Git branch used for production deployments? [main]: &gt;What is the template file path? [template.yaml]: &gt;We use the stage name to automatically retrieve the bootstrapped resources created when you ran `sam pipeline bootstrap`.Here are the stage configuration names detected in .aws-sam/pipeline/pipelineconfig.toml: 1 - testWhat is the name of stage 1 (as provided during the bootstrapping)?Select an index or enter the stage name: &gt; 1What is the sam application stack name for stage 1? [sam-app]: &gt;Stage 1 configured successfully (you only have one stage).To deploy this template and connect to the main git branch, run this against the leading account:`sam deploy -t codepipeline.yaml --stack-name &lt;stack-name&gt; --capabilities=CAPABILITY_IAM`.SUMMARYWe will generate a pipeline config file based on the following information: What is the Git provider?: GitHub What is the full repository id (Example: some-user/my-repo)?: alazaroc/aws-sam-app What is the Git branch used for production deployments?: main What is the template file path?: template.yaml What is the name of stage 1 (as provided during the bootstrapping)?Select an index or enter the stage name: 1 What is the sam application stack name for stage 1?: sam-app What is the pipeline execution role ARN for stage 1?: arn:aws:iam::xxxxxxxxxxxx:role/aws-sam-cli-managed-test-pip-PipelineExecutionRole-1RT9YMZ60U7L8 What is the CloudFormation execution role ARN for stage 1?: arn:aws:iam::xxxxxxxxxxxx:role/aws-sam-cli-managed-test-CloudFormationExecutionR-FA52519RHRDD What is the S3 bucket name for artifacts for stage 1?: aws-sam-cli-managed-test-pipeline-artifactsbucket-jf6hixn3rx29 What is the ECR repository URI for stage 1?: What is the AWS region for stage 1?: eu-west-1Successfully created the pipeline configuration file(s): - codepipeline.yaml - assume-role.sh - pipeline/buildspec_unit_test.yml - pipeline/buildspec_build_package.yml - pipeline/buildspec_integration_test.yml - pipeline/buildspec_feature.yml - pipeline/buildspec_deploy.ymlNow we have the new files in our project that CodePipeline will use to deploy our code:Step 3: Commit your pipeline configuration to GitThis step is necessary to ensure your CI/CD system is aware of your pipeline configuration and will run when changes are committed.Step 4: Deploy your pipelineFor AWS CodePipeline you have to deploy the pipeline running sam deploy -t codepipeline.yaml --stack-name &lt;pipeline-stack-name&gt; --capabilities=CAPABILITY_IAM --region &lt;region-X&gt; Don’t set the same stack name as your SAM application because doing so will overwrite your application’s stack (and delete your application resources).&gt; sam deploy -t codepipeline.yaml --stack-name sam-app-pipeline --capabilities=CAPABILITY_IAM Deploying with following values =============================== Stack name : sam-app-pipeline Region : eu-west-1 Confirm changeset : True Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-19wv6mek3hxyw Capabilities : [\"CAPABILITY_IAM\"] Parameter overrides : {} Signing Profiles : {}Initiating deployment=====================File with same data already exists at sam-app/f421d3ae8f9c85b69c81f641d104e1eb.template, skipping uploadWaiting for changeset to be created..CloudFormation stack changeset-------------------------------------------------------------------------------------------------------------------------Operation LogicalResourceId ResourceType Replacement-------------------------------------------------------------------------------------------------------------------------+ Add CodeBuildProjectBuildAndPack AWS::CodeBuild::Project N/A+ Add CodeBuildProjectDeploy AWS::CodeBuild::Project N/A+ Add CodeBuildServiceRole AWS::IAM::Role N/A+ Add CodePipelineExecutionRole AWS::IAM::Role N/A+ Add CodeStarConnection AWS::CodeStarConnections::Co N/A+ Add PipelineArtifactsBucketPolic AWS::S3::BucketPolicy N/A+ Add PipelineArtifactsBucket AWS::S3::Bucket N/A+ Add PipelineArtifactsLoggingBuck AWS::S3::BucketPolicy N/A+ Add PipelineArtifactsLoggingBuck AWS::S3::Bucket N/A+ Add PipelineStackCloudFormationE AWS::IAM::Role N/A+ Add Pipeline AWS::CodePipeline::Pipeline N/A-------------------------------------------------------------------------------------------------------------------------Changeset created successfully. arn:aws:cloudformation:eu-west-1:xxxxxxxxxxxx:changeSet/samcli-deploy1649184605/5c7e3379-74d0-486e-a387-99837b6ab74bPreviewing CloudFormation changeset before deployment======================================================Deploy this changeset? [y/N]: &gt; Y2022-04-05 20:50:13 - Waiting for stack create/update to completeCloudFormation events from stack operations-------------------------------------------------------------------------------------------------------------------------ResourceStatus ResourceType LogicalResourceId ResourceStatusReason-------------------------------------------------------------------------------------------------------------------------CREATE_IN_PROGRESS AWS::S3::Bucket PipelineArtifactsLoggingBuck -CREATE_IN_PROGRESS AWS::IAM::Role PipelineStackCloudFormationE -CREATE_IN_PROGRESS AWS::CodeStarConnections::Co CodeStarConnection -CREATE_IN_PROGRESS AWS::IAM::Role PipelineStackCloudFormationE Resource creation InitiatedCREATE_IN_PROGRESS AWS::S3::Bucket PipelineArtifactsLoggingBuck Resource creation InitiatedCREATE_COMPLETE AWS::CodeStarConnections::Co CodeStarConnection -CREATE_IN_PROGRESS AWS::CodeStarConnections::Co CodeStarConnection Resource creation InitiatedCREATE_COMPLETE AWS::IAM::Role PipelineStackCloudFormationE -CREATE_COMPLETE AWS::S3::Bucket PipelineArtifactsLoggingBuck -CREATE_IN_PROGRESS AWS::S3::BucketPolicy PipelineArtifactsLoggingBuck -CREATE_IN_PROGRESS AWS::S3::Bucket PipelineArtifactsBucket -CREATE_IN_PROGRESS AWS::S3::Bucket PipelineArtifactsBucket Resource creation InitiatedCREATE_COMPLETE AWS::S3::BucketPolicy PipelineArtifactsLoggingBuck -CREATE_IN_PROGRESS AWS::S3::BucketPolicy PipelineArtifactsLoggingBuck Resource creation InitiatedCREATE_COMPLETE AWS::S3::Bucket PipelineArtifactsBucket -CREATE_IN_PROGRESS AWS::IAM::Role CodeBuildServiceRole -CREATE_IN_PROGRESS AWS::IAM::Role CodeBuildServiceRole Resource creation InitiatedCREATE_COMPLETE AWS::IAM::Role CodeBuildServiceRole -CREATE_IN_PROGRESS AWS::CodeBuild::Project CodeBuildProjectBuildAndPack -CREATE_IN_PROGRESS AWS::CodeBuild::Project CodeBuildProjectDeploy -CREATE_IN_PROGRESS AWS::CodeBuild::Project CodeBuildProjectDeploy Resource creation InitiatedCREATE_IN_PROGRESS AWS::CodeBuild::Project CodeBuildProjectBuildAndPack Resource creation InitiatedCREATE_COMPLETE AWS::CodeBuild::Project CodeBuildProjectDeploy -CREATE_COMPLETE AWS::CodeBuild::Project CodeBuildProjectBuildAndPack -CREATE_IN_PROGRESS AWS::IAM::Role CodePipelineExecutionRole -CREATE_IN_PROGRESS AWS::IAM::Role CodePipelineExecutionRole Resource creation InitiatedCREATE_COMPLETE AWS::IAM::Role CodePipelineExecutionRole -CREATE_IN_PROGRESS AWS::S3::BucketPolicy PipelineArtifactsBucketPolic -CREATE_IN_PROGRESS AWS::S3::BucketPolicy PipelineArtifactsBucketPolic Resource creation InitiatedCREATE_IN_PROGRESS AWS::CodePipeline::Pipeline Pipeline -CREATE_COMPLETE AWS::S3::BucketPolicy PipelineArtifactsBucketPolic -CREATE_IN_PROGRESS AWS::CodePipeline::Pipeline Pipeline Resource creation InitiatedCREATE_COMPLETE AWS::CodePipeline::Pipeline Pipeline -CREATE_COMPLETE AWS::CloudFormation::Stack sam-app-pipeline --------------------------------------------------------------------------------------------------------------------------CloudFormation outputs from deployed stack---------------------------------------------------------------------------------------------------------------------------Outputs---------------------------------------------------------------------------------------------------------------------------Key CodeStarConnectionArnDescription The Arn of AWS CodeStar Connection used to connect to external code repositories.Value arn:aws:codestar-connections:eu-west-1:xxxxxxxxxxxx:connection/81d48b76-1ae7-4d69-b6eb-11eea6acf94a---------------------------------------------------------------------------------------------------------------------------Successfully created/updated stack - sam-app-pipeline in eu-west-1A new stack has been created to deploy our AWS CodePipeline: But the first execution has failed We can see in the CodePipeline flow all steps created: Source: integrated with GitHub as we indicated before UpdatePipeline: the pipeline can update itself BuildAndPackage: the SAM application is built, packaged, and uploaded (with AWS CodeBuild service) DeployTest: the SAM application is deployed (with AWS CodeBuild service)The cause of the error was that the connection between GitHub and AWS must be confirmed after being created:Step 5: Connect your Git repository with your CI/CD system If you are using GitHub or Bitbucket, after running the sam deploy command, you need to complete the pending connection in the Settings/Connection section of the Developer Tools. In addition, you could store a copy of the CodeStarConnectionArn from the output of the sam deploy command, because you will need it if you want to use AWS CodePipeline with another branch than main.By accessing the Settings/Connections in the Developer Tools, you can validate that the connection is pending approval:After activating it, we run the pipeline again (by clicking on the Release change button) and now the pipeline ends correctly.Update CI/CD steps to SAM projectNote that now we have a pipeline that first checks for changes in the pipeline itself and then checks the code and deploys the resources.Step 1: Update steps in the pipeline automatically With this pipeline configuration (with UpdatePipeline step) all the changes that we make in the pipeline will be updated automatically.We want to test the automatic update of the pipeline if we make some changes.So, let’s change the pipeline steps to: delete the UpdatePipeline step add the UnitTest stepTo do this, we have to update the codepipeline.yaml file with the necessary changes: comment all the UpdatePipeline step code uncomment all the UnitTest step codeAnd after that, commit the changes and push them to the repository:When we push the changes, the UpdatePipeline step executes an update on the sam-app-pipeline stack (in the CloudFormation service) and it will update the pipeline definition.As we expected, the pipeline has updated itself and now we have UnitTest step but not UpdatePipeline.Step 2: Update steps in the pipeline manually If you remove the UpdatePipeline step, when you push a change to the repository the pipeline won’t be updated, so you have to run manually the update of the pipeline.To update the pipeline we have to run again the command sam deploy -t codepipeline.yaml --stack-name sam-app-pipeline --capabilities=CAPABILITY_IAM&gt; sam deploy -t codepipeline.yaml --stack-name sam-app-pipeline --capabilities=CAPABILITY_IAM Deploying with following values =============================== Stack name : sam-app-pipeline Region : eu-west-1 Confirm changeset : True Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 Capabilities : [\"CAPABILITY_IAM\"] Parameter overrides : {} Signing Profiles : {}Initiating deployment=====================Uploading to sam-app/23360d9a8d229e9ea24f2e1a53ea8b00.template 15967 / 15967 (100.00%)Waiting for changeset to be created..CloudFormation stack changeset-------------------------------------------------------------------------------------------------Operation LogicalResourceId ResourceType Replacement-------------------------------------------------------------------------------------------------* Modify CodeBuildProjectBuildA AWS::CodeBuild::Projec Conditional* Modify CodeBuildProjectDeploy AWS::CodeBuild::Projec Conditional* Modify CodeBuildServiceRole AWS::IAM::Role False* Modify CodePipelineExecutionR AWS::IAM::Role False* Modify CodeStarConnection AWS::CodeStarConnectio False* Modify PipelineArtifactsBucke AWS::S3::BucketPolicy False* Modify PipelineArtifactsBucke AWS::S3::Bucket False* Modify PipelineArtifactsLoggi AWS::S3::BucketPolicy False* Modify PipelineArtifactsLoggi AWS::S3::Bucket False* Modify PipelineStackCloudForm AWS::IAM::Role False* Modify Pipeline AWS::CodePipeline::Pip False- Delete CodeBuildProjectUnitTe AWS::CodeBuild::Projec N/A-------------------------------------------------------------------------------------------------Changeset created successfully. arn:aws:cloudformation:eu-west-1:xxxxxxxxxxxx:changeSet/samcli-deploy1649503303/4d23550d-27da-458d-ba18-f3be6db7ca54Previewing CloudFormation changeset before deployment======================================================Deploy this changeset? [y/N]: &gt; Y2022-04-09 13:22:01 - Waiting for stack create/update to completeCloudFormation events from stack operations-------------------------------------------------------------------------------------------------ResourceStatus ResourceType LogicalResourceId ResourceStatusReason-------------------------------------------------------------------------------------------------UPDATE_COMPLETE AWS::S3::Bucket PipelineArtifactsLoggi -UPDATE_COMPLETE AWS::IAM::Role PipelineStackCloudForm -UPDATE_COMPLETE AWS::CodeStarConnectio CodeStarConnection -UPDATE_COMPLETE AWS::S3::Bucket PipelineArtifactsBucke -UPDATE_COMPLETE AWS::S3::BucketPolicy PipelineArtifactsLoggi -UPDATE_COMPLETE AWS::IAM::Role CodeBuildServiceRole -UPDATE_COMPLETE AWS::CodeBuild::Projec CodeBuildProjectBuildA -UPDATE_COMPLETE AWS::CodeBuild::Projec CodeBuildProjectDeploy -UPDATE_IN_PROGRESS AWS::IAM::Role CodePipelineExecutionR -UPDATE_COMPLETE AWS::IAM::Role CodePipelineExecutionR -UPDATE_COMPLETE AWS::S3::BucketPolicy PipelineArtifactsBucke -UPDATE_IN_PROGRESS AWS::CodePipeline::Pip Pipeline -UPDATE_COMPLETE AWS::CodePipeline::Pip Pipeline -UPDATE_COMPLETE_CLEANU AWS::CloudFormation::S sam-app-pipeline -DELETE_IN_PROGRESS AWS::CodeBuild::Projec CodeBuildProjectUnitTe -UPDATE_COMPLETE AWS::CloudFormation::S sam-app-pipeline -DELETE_COMPLETE AWS::CodeBuild::Projec CodeBuildProjectUnitTe --------------------------------------------------------------------------------------------------CloudFormation outputs from deployed stack-------------------------------------------------------------------------------------------------Outputs-------------------------------------------------------------------------------------------------Key CodeStarConnectionArnDescription The Arn of AWS CodeStar Connection used to connect to external coderepositories.Value arn:aws:codestar-connections:eu-west-1:xxxxxxxxxxxx:connection/cc4ce462-f77f-43ab-b63c-8012ef6a467e-------------------------------------------------------------------------------------------------Successfully created/updated stack - sam-app-pipeline in eu-west-1And the pipeline will be updated:Summary of the current stateAfter all changes that we did: Now, we have 3 steps in our CodePipeline: Source: integrated with GitHub as we indicated before BuildAndPackage: the SAM application is built, packaged, and uploaded (with AWS CodeBuild service) DeployTest: the SAM application is deployed (with AWS CodeBuild service) To summarize the current state: If we change the SAM application and update the code in our GitHub repository, the pipeline will update the resources in our AWS account. If we want to update the pipeline itself, we have to update the specific pipeline files in our SAM application, and then run manually the command sam deploy -t codepipeline.yaml --stack-name sam-app-pipeline --capabilities=CAPABILITY_IAM. Clean upWe have created 4 stacks in CloudFormation related to SAM: sam-app: application code sam-app-pipeline: codepipeline aws-sam-cli-managed-test-pipeline-resources: test stage resources aws-sam-cli-managed-default: general sam resources If you want to use SAM in the future you could keep the aws-sam-cli-managed-default stack.You have several ways to delete your resources: AWS CloudFormation service AWS CLI WS SAM CLI If you execute the command sam delete, it only will delete the main stack (sam-app) but not the CI/CD pipeline or the stage resources stack.Comment this post I have temporarily added the comments section to the post here. In the future, I will add it in a better way and include all the validated comments (I guess that I will have to make a filter to avoid spam) Submit Submitting... Failed to submit the form Please, try again. Thank you! I will add your comment in a future. " }, { "title": "How to create serverless applications with SAM", "url": "/posts/how-to-create-serverless-applications-with-sam/", "categories": "How-to, Serverless, IaC", "tags": "sam, cloudformation, lambda, github", "date": "2022-04-09 15:07:00 +0200", "snippet": "IntroductionThe AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS.A serverless application is more than just a Lambda Function. It is a combination of Lambda functions, event sources, APIs, databases, and other resources that work together to perform tasks.SAM is an extension of AWS CloudFormation but SAM is streamlined and specifically designed for Serverless resources. SAM is the specific IaC solution of AWS for defining and deploying Lambda applications without the need to ever touch the AWS Console.Benefits of SAM Local Testing and Debugging With the aws sam cli you can execute and test your serverless applications on your local (by mounting a docker image and running the code) Extension of AWS Cloud Formation You get reliability on the deployment capabilities You are also able to use in the SAM YAML template all of the resources that are available in CloudFormation Single Deployment Configuration You can easily manage all your necessary resources in one single place that belongs to the same stack Built-in best practices You can define and deploy your infrastructure as config you can enforce code reviews you can enable safe deployments through CodeDeploy you can enable tracing by using AWS X-Ray Deep integration with development tools AWS Serverless Application Repository: discover new applications AWS Cloud9 IDE: For authoring, testing, and debugging CodeBuild, CodeDeploy, and CodePipeline: To build a deployment pipeline AWS CodeStar: To get started with a project structure, code repository, and a CI/CD pipeline that’s automatically configured for you BasicsTo understand the code structure of the SAM projects, five files are particularly important: src/handlers/file.js: src folder contains the code for the application’s Lambda Function. __tests__/unit/handlers/file.test.js: test folder contains the unit tests for the application code. events/file.json: events folder contains the invocation events that you can use to invoke the function. template.yaml: This file contains the AWS SAM template that defines your application’s AWS resources. package.json: This file of NodeJS contains the application dependencies and is used for the sam build. If you are using Python language instead of NodeJS, the file will be requirements.txt. The location of Lambda Functions or Unit Tests could change! In some examples, a folder with the name of the lambda is created and inside is located the lambda function and the unit tests.Example of Lambda FunctionThis is the template.yaml content for a Lambda Function:# The AWSTemplateFormatVersion identifies the capabilities of the template# https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/format-version-structure.htmlAWSTemplateFormatVersion: 2010-09-09Description: &gt;- sam-app# Transform section specifies one or more macros that AWS CloudFormation uses to process your template# https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-section-structure.htmlTransform:- AWS::Serverless-2016-10-31# Resources declares the AWS resources that you want to include in the stack# https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resources-section-structure.htmlResources: # Each Lambda function is defined by properties: # https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction # This is a Lambda function config associated with the source code: hello-from-lambda.js helloFromLambdaFunction: Type: AWS::Serverless::Function Properties: Handler: src/handlers/hello-from-lambda.helloFromLambdaHandler Runtime: nodejs14.x Architectures: - x86_64 MemorySize: 128 Timeout: 100 Description: A Lambda function that returns a static string. Policies: # Give Lambda basic execution Permission to the helloFromLambda - AWSLambdaBasicExecutionRoleAdd an API GatewayTo add an API (Amazon API Gateway) to your Lambda Function you will have to update inside the properties by adding Events as follows:Resources: HelloWorldFunction: Properties: ... Events: HelloWorld: Type: Api Properties: Path: / Method: getAnd you have to create in events folder the json definition of the method:{ \"httpMethod\": \"GET\"}Add an scheduled eventInclude a scheduled event is pretty similar to add an API.Resources: HelloWorldFunction: Properties: ... Events: CloudWatchEvent: Type: Schedule Properties: Schedule: cron(0 * * * ? *)And you have to create in events folder the json definition of the rule:{ \"id\": \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\", \"detail-type\": \"Scheduled Event\", \"source\": \"aws.events\", \"account\": \"\", \"time\": \"1970-01-01T00:00:00Z\", \"region\": \"us-west-2\", \"resources\": [ \"arn:aws:events:us-west-2:xxxxxxxxxxxx:rule/ExampleRule\" ], \"detail\": {}}More examples I recommend that you run aws sam init and ty to create different projects from the templates.More information about template anatomy here.Prerequisites AWS CLI how to install it how configure it AWS SAM CLI (here)SAM applicationTo keep it simple, we will create a SAM application from a quick start template using the standalone function but you could try a different template if you wish. The code example is available here. If you want to see the step by step you can check the commit history, where you can find the evolution of the application through the steps explained in the following lines.These are all the steps that I want to show you in this article: Step 1: Download a sample SAM application Step 2 (Optional): Test your application locally Step 3 (Optional): Unit test Step 4: Build your application Step 5: Deploy manually your application with the CLI Step 6 (Optional): AWS SAM Accelerate (Preview) - SyncStep 1: Download a sample SAM applicationThe first step is to create our application through a quick start template: Standalone function.To create a new application from a template we run the sam init command.&gt; sam initWhich template source would you like to use? 1 - AWS Quick Start Templates 2 - Custom Template LocationChoice: &gt; 1Choose an AWS Quick Start application template 1 - Hello World Example 2 - Multi-step workflow 3 - Serverless API 4 - Scheduled task 5 - Standalone function 6 - Data processing 7 - Infrastructure event management 8 - Machine LearningTemplate: &gt; 5Which runtime would you like to use? 1 - dotnetcore3.1 2 - nodejs14.x 3 - nodejs12.xRuntime: &gt; 2Based on your selections, the only Package type available is Zip.We will proceed to selecting the Package type as Zip.Based on your selections, the only dependency manager available is npm.We will proceed copying the template using npm.Project name [sam-app]: &gt; sam-appCloning from https://github.com/aws/aws-sam-cli-app-templates (process may take a moment) ----------------------- Generating application: ----------------------- Name: sam-app Runtime: nodejs14.x Architectures: x86_64 Dependency Manager: npm Application Template: quick-start-from-scratch Output Directory: . Next steps can be found in the README file at ./sam-app/README.md Commands you can use next ========================= [*] Create pipeline: cd sam-app &amp;&amp; sam pipeline init --bootstrap [*] Test Function in the Cloud: sam sync --stack-name {stack-name} --watch Note that at the end of the command line messages, Commands you can use next appears where other SAM CLI commands are suggested as the next steps to execute.This is the basic application that has been created (only with one lambda function for easy understanding):Note that we have 4 of the 5 files that we reviewed before: src/handlers/hello-from-lambda.js _test_/unit/handlers/hello-from-lambda.test.js template.yaml package.jsonWe don’t have the folder events because we only create one simple Lambda Function with no event integrationsStep 2 (Optional): Test your application locallyThe AWS SAM CLI provides the sam local command to run your application using Docker containers that simulate the execution environment of Lambda.Invoke your Lambda function running sam local invoke:&gt; sam local invokeInvoking src/handlers/hello-from-lambda.helloFromLambdaHandler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.43.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/sam/sam-app as /var/task:ro,delegated inside runtime containerSTART RequestId: c8b494ab-a1db-48b7-8f50-f9a93e1305ef Version: $LATEST2022-04-08T18:55:39.230Z c8b494ab-a1db-48b7-8f50-f9a93e1305ef INFO Hello from Lambda!END RequestId: c8b494ab-a1db-48b7-8f50-f9a93e1305efREPORT RequestId: c8b494ab-a1db-48b7-8f50-f9a93e1305ef Init Duration: 1.55 ms Duration: 363.29 ms Billed Duration: 364 ms Memory Size: 128 MB Max Memory Used: 128 MB\"Hello from Lambda!\"We received the response \"Hello from Lambda!\" and more useful information (Duration, Billed Duration, Memory Size, or Max Memory Used).If you have more than one Lambda Function, you must add the name which appears in the template.yaml file.&gt; sam local invoke \"helloFromLambdaFunction\"Invoking src/handlers/hello-from-lambda.helloFromLambdaHandler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.43.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/sam/sam-app as /var/task:ro,delegated inside runtime containerSTART RequestId: dc41d97d-0867-4516-94d9-d1830192565e Version: $LATEST2022-04-08T18:56:44.834Z dc41d97d-0867-4516-94d9-d1830192565e INFO Hello from Lambda!\"Hello from Lambda!\"END RequestId: dc41d97d-0867-4516-94d9-d1830192565eREPORT RequestId: dc41d97d-0867-4516-94d9-d1830192565e Init Duration: 0.49 ms Duration: 224.97 ms Billed Duration: 225 ms Memory Size: 128 MB Max Memory Used: 128 MB You also can test an API locally if your SAM project includes it. You should run sam local start-api command, which starts up a local endpoint that replicates your REST API endpoint.Step 3 (Optional): Unit testTests are defined in the __tests__ folder in this project. Use npm to install the Jest test framework and run unit tests.&gt; npm install...&gt; npm run testreplaced-by-user-input@0.0.1 testjestjest-haste-map: Haste module naming collision: replaced-by-user-input The following files share their name; please adjust your hasteImpl: * &lt;rootDir&gt;/package.json * &lt;rootDir&gt;/.aws-sam/build/helloFromLambdaFunction/package.json PASS __tests__/unit/handlers/hello-from-lambda.test.js ● Console console.info Hello from Lambda! at Object.helloFromLambdaHandler (src/handlers/hello-from-lambda.js:9:13) PASS .aws-sam/build/helloFromLambdaFunction/__tests__/unit/handlers/hello-from-lambda.test.js ● Console console.info Hello from Lambda! at Object.helloFromLambdaHandler (.aws-sam/build/helloFromLambdaFunction/src/handlers/hello-from-lambda.js:9:13)Test Suites: 2 passed, 2 totalTests: 2 passed, 2 totalSnapshots: 0 totalTime: 1.854 sRan all test suites.Step 4: Build your applicationThe sam build command builds any dependencies that your application has, and copies your application source code to folders under .aws-sam/build to be zipped and uploaded to Lambda.&gt; sam buildBuilding codeuri: /Users/alazaroc/Documents/MyProjects/github/aws/sam/sam-app runtime: nodejs14.x metadata: {} architecture: x86_64 functions: ['helloFromLambdaFunction']Running NodejsNpmBuilder:NpmPackRunning NodejsNpmBuilder:CopyNpmrcAndLockfileRunning NodejsNpmBuilder:CopySourceRunning NodejsNpmBuilder:NpmInstallRunning NodejsNpmBuilder:CleanUpNpmrcRunning NodejsNpmBuilder:LockfileCleanUpBuild SucceededBuilt Artifacts : .aws-sam/buildBuilt Template : .aws-sam/build/template.yamlCommands you can use next=========================[*] Invoke Function: sam local invoke[*] Test Function in the Cloud: sam sync --stack-name {stack-name} --watch[*] Deploy: sam deploy --guidedThese are the new files of our SAM project:Step 5: Deploy manually your application with the CLINow we want to deploy our application. We will do it now manually from CLI, although in this other article I will do it with a pipeline (automatically). Remember that AWS SAM uses AWS CloudFormation as the underlying deployment mechanism.As we don’t have a configuration file containing all the values, we are going to create one. We run the sam deploy -- guided command which will search as a first step if a samconfig.toml file exists and if not the AWS SAM CLI will ask us about the necessary information to deploy our application. The sam deploy command will package and upload the application artifacts to the S3 bucket, and deploys the application using AWS CloudFormation&gt; sam deploy --guidedConfiguring SAM deploy====================== Looking for config file [samconfig.toml] : Not found Setting default arguments for 'sam deploy' ========================================= Stack Name [sam-app]: AWS Region [eu-west-1]: #Shows you resources changes to be deployed and require a 'Y' to initiate deploy Confirm changes before deploy [y/N]: &gt; Y #SAM needs permission to be able to create roles to connect to the resources in your template Allow SAM CLI IAM role creation [Y/n]: &gt; #Preserves the state of previously provisioned resources when an operation fails Disable rollback [y/N]: &gt; Save arguments to configuration file [Y/n]: &gt; SAM configuration file [samconfig.toml]: &gt; SAM configuration environment [default]: &gt; Looking for resources needed for deployment: Creating the required resources... Successfully created! Managed S3 bucket: aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 A different default S3 bucket can be set in samconfig.toml Saved arguments to config file Running 'sam deploy' for future deployments will use the parameters saved above. The above parameters can be changed by modifying samconfig.toml Learn more about samconfig.toml syntax at https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-config.htmlUploading to sam-app/c606de95995c9e6d65f310f130ccc787 5777 / 5777 (100.00%) Deploying with following values =============================== Stack name : sam-app Region : eu-west-1 Confirm changeset : True Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 Capabilities : [\"CAPABILITY_IAM\"] Parameter overrides : {} Signing Profiles : {}Initiating deployment=====================Uploading to sam-app/c3a5d8cf01d6af391d2863628b67fbbe.template 659 / 659 (100.00%)Waiting for changeset to be created..CloudFormation stack changeset-----------------------------------------------------------------------------------------------------------------------------------------------------Operation LogicalResourceId ResourceType Replacement-----------------------------------------------------------------------------------------------------------------------------------------------------+ Add helloFromLambdaFunctionRole AWS::IAM::Role N/A+ Add helloFromLambdaFunction AWS::Lambda::Function N/A-----------------------------------------------------------------------------------------------------------------------------------------------------Changeset created successfully. arn:aws:cloudformation:eu-west-1:xxxxxxxxxxxx:changeSet/samcli-deploy1649445168/3355bdea-6549-4a2b-b486-94befe703b4dPreviewing CloudFormation changeset before deployment======================================================Deploy this changeset? [y/N]: &gt; Y2022-04-08 21:13:08 - Waiting for stack create/update to completeCloudFormation events from stack operations-----------------------------------------------------------------------------------------------------------------------------------------------------ResourceStatus ResourceType LogicalResourceId ResourceStatusReason-----------------------------------------------------------------------------------------------------------------------------------------------------CREATE_IN_PROGRESS AWS::IAM::Role helloFromLambdaFunctionRole -CREATE_IN_PROGRESS AWS::IAM::Role helloFromLambdaFunctionRole Resource creation InitiatedCREATE_COMPLETE AWS::IAM::Role helloFromLambdaFunctionRole -CREATE_IN_PROGRESS AWS::Lambda::Function helloFromLambdaFunction -CREATE_IN_PROGRESS AWS::Lambda::Function helloFromLambdaFunction Resource creation InitiatedCREATE_COMPLETE AWS::Lambda::Function helloFromLambdaFunction -CREATE_COMPLETE AWS::CloudFormation::Stack sam-app ------------------------------------------------------------------------------------------------------------------------------------------------------Successfully created/updated stack - sam-app in eu-west-1Remember that the executed command will create the samconfig.toml file in our project to save the deployment configuration and be able to repeat it without configuration.From now on, to deploy our SAM project we just need to run the sam deploy command, so we run it but if we have no changes, the deployment will fail:&gt; sam deployFile with same data already exists at sam-app/c606de95995c9e6d65f310f130ccc787, skipping upload Deploying with following values =============================== Stack name : sam-app Region : eu-west-1 Confirm changeset : True Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 Capabilities : [\"CAPABILITY_IAM\"] Parameter overrides : {} Signing Profiles : {}Initiating deployment=====================File with same data already exists at sam-app/c3a5d8cf01d6af391d2863628b67fbbe.template, skipping uploadWaiting for changeset to be created..Error: No changes to deploy. Stack sam-app is up to dateStep 6 (Optional): AWS SAM Accelerate (Preview) - SyncWe already have deployed our application in the cloud and you may want to synchronize the changes, i.e. deploy the changes in real-time when we save the changes (without running the deploy command). The sync command deploys your local changes to the AWS Cloud. Use sync to build, package, and deploy changes to your development environment as you iterate on your application. As a best practice, run sam sync after you finish iterating on your application to sync changes to your AWS CloudFormation stack. Be careful if you use this functionality. First it is in preview and also as you will see in the next lines in the console: “The SAM CLI will use the AWS Lambda, Amazon API Gateway, and AWS StepFunctions APIs to upload your code without performing a CloudFormation deployment. This will cause drift in your CloudFormation stack.” The sync command should only be used against a development stack.&gt; sam sync --stack-name sam-app --watchManaged S3 bucket: aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0Default capabilities applied: ('CAPABILITY_NAMED_IAM', 'CAPABILITY_AUTO_EXPAND')To override with customized capabilities, use --capabilities flag or set it in samconfig.tomlThis feature is currently in beta. Visit the docs page to learn more about the AWS Beta terms https://aws.amazon.com/service-terms/.The SAM CLI will use the AWS Lambda, Amazon API Gateway, and AWS StepFunctions APIs to upload your code withoutperforming a CloudFormation deployment. This will cause drift in your CloudFormation stack.**The sync command should only be used against a development stack**.Confirm that you are synchronizing a development stack and want to turn on beta features.Enter Y to proceed with the command, or enter N to cancel: [y/N]: &gt; YExperimental features are enabled for this session.Visit the docs page to learn more about the AWS Beta terms https://aws.amazon.com/service-terms/.Queued infra sync. Wating for in progress code syncs to complete...Starting infra sync.Manifest file is changed (new hash: c448eb733590e1cea85b58f147c47f01) or dependency folder (.aws-sam/deps/c14e7b11-e157-4aea-a19b-97b33b39cef5) is missing for c14e7b11-e157-4aea-a19b-97b33b39cef5, downloading dependencies and copying/building sourceBuilding codeuri: /Users/alazaroc/Documents/MyProjects/github/aws/sam/sam-app runtime: nodejs14.x metadata: {} architecture: x86_64 functions: ['helloFromLambdaFunction']Running NodejsNpmBuilder:NpmPackRunning NodejsNpmBuilder:CopyNpmrcAndLockfileRunning NodejsNpmBuilder:CopySourceRunning NodejsNpmBuilder:NpmInstallRunning NodejsNpmBuilder:CleanUpClean up action: .aws-sam/deps/c14e7b11-e157-4aea-a19b-97b33b39cef5 does not exist and will be skipped.Running NodejsNpmBuilder:MoveDependenciesRunning NodejsNpmBuilder:CleanUpNpmrcRunning NodejsNpmBuilder:LockfileCleanUpRunning NodejsNpmBuilder:LockfileCleanUpBuild SucceededSuccessfully packaged artifacts and wrote output template to file /var/folders/wq/bz6xngtx5h3f5gf8py3kf28c0000gn/T/tmp6076iix4.Execute the following command to deploy the packaged templatesam deploy --template-file /var/folders/wq/bz6xngtx5h3f5gf8py3kf28c0000gn/T/tmp6076iix4 --stack-name &lt;YOUR STACK NAME&gt; Deploying with following values =============================== Stack name : sam-app Region : eu-west-1 Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 Capabilities : [\"CAPABILITY_NAMED_IAM\", \"CAPABILITY_AUTO_EXPAND\"] Parameter overrides : {} Signing Profiles : nullInitiating deployment=====================2022-04-08 21:34:02 - Waiting for stack create/update to completeCloudFormation events from stack operations-----------------------------------------------------------------------------------------------------------------------------------------------------ResourceStatus ResourceType LogicalResourceId ResourceStatusReason-----------------------------------------------------------------------------------------------------------------------------------------------------UPDATE_IN_PROGRESS AWS::CloudFormation::Stack sam-app Transformation succeededCREATE_IN_PROGRESS AWS::CloudFormation::Stack AwsSamAutoDependencyLayerNestedStac -CREATE_IN_PROGRESS AWS::CloudFormation::Stack AwsSamAutoDependencyLayerNestedStac Resource creation InitiatedCREATE_COMPLETE AWS::CloudFormation::Stack AwsSamAutoDependencyLayerNestedStac -UPDATE_IN_PROGRESS AWS::Lambda::Function helloFromLambdaFunction -UPDATE_COMPLETE AWS::Lambda::Function helloFromLambdaFunction -UPDATE_COMPLETE_CLEANUP_IN_PROGRESS AWS::CloudFormation::Stack sam-app -UPDATE_COMPLETE AWS::CloudFormation::Stack sam-app ------------------------------------------------------------------------------------------------------------------------------------------------------Stack update succeeded. Sync infra completed.{'StackId': 'arn:aws:cloudformation:eu-west-1:xxxxxxxxxxxx:stack/sam-app/e17d9780-b76f-11ec-a123-02591afab591', 'ResponseMetadata': {'RequestId': 'c726d9ed-ba77-473c-9155-9e05cc7ee0c7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c726d9ed-ba77-473c-9155-9e05cc7ee0c7', 'content-type': 'text/xml', 'content-length': '377', 'date': 'Fri, 08 Apr 2022 19:34:02 GMT'}, 'RetryAttempts': 0}}Infra sync completed.The console still listens for changes and if we change our lambda code and save it:The console will be updated automatically as follow:Syncing Lambda Function helloFromLambdaFunction...Manifest is not changed for c14e7b11-e157-4aea-a19b-97b33b39cef5, running incremental buildBuilding codeuri: /Users/alazaroc/Documents/MyProjects/github/aws/sam/sam-app runtime: nodejs14.x metadata: {} architecture: x86_64 functions: ['helloFromLambdaFunction']download_dependencies is False and dependencies_dir is None. Copying the source files into the artifacts directory.Running NodejsNpmBuilder:NpmPackRunning NodejsNpmBuilder:CopyNpmrcAndLockfileRunning NodejsNpmBuilder:CopySourceRunning NodejsNpmBuilder:CleanUpNpmrcRunning NodejsNpmBuilder:LockfileCleanUpRunning NodejsNpmBuilder:LockfileCleanUpFinished syncing Lambda Function helloFromLambdaFunction.When you stop it (control + C) in the console it will appear:Shutting down sync watch...Sync watch stopped. When you executed the sync command, a nested stack associated with your main stack (sam-app) was created: And when the console stops being synchronized, this nested stack is NOT deleted.How to remove the nested stack created with the sync command?You have to run the sam deploy command again:&gt; sam deployFile with same data already exists at sam-app/c606de95995c9e6d65f310f130ccc787, skipping upload Deploying with following values =============================== Stack name : sam-app Region : eu-west-1 Confirm changeset : True Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 Capabilities : [\"CAPABILITY_IAM\"] Parameter overrides : {} Signing Profiles : {}Initiating deployment=====================File with same data already exists at sam-app/c3a5d8cf01d6af391d2863628b67fbbe.template, skipping uploadWaiting for changeset to be created..CloudFormation stack changeset-----------------------------------------------------------------------------------------------------------------------------------------------------Operation LogicalResourceId ResourceType Replacement-----------------------------------------------------------------------------------------------------------------------------------------------------* Modify helloFromLambdaFunction AWS::Lambda::Function False- Delete AwsSamAutoDependencyLayerNestedStac AWS::CloudFormation::Stack N/A-----------------------------------------------------------------------------------------------------------------------------------------------------Changeset created successfully. arn:aws:cloudformation:eu-west-1:xxxxxxxxxxxx:changeSet/samcli-deploy1649447607/790db35a-524f-48c5-af5c-1239e1b8fe92Previewing CloudFormation changeset before deployment======================================================Deploy this changeset? [y/N]: &gt; Y2022-04-08 21:53:42 - Waiting for stack create/update to completeCloudFormation events from stack operations-----------------------------------------------------------------------------------------------------------------------------------------------------ResourceStatus ResourceType LogicalResourceId ResourceStatusReason-----------------------------------------------------------------------------------------------------------------------------------------------------UPDATE_IN_PROGRESS AWS::Lambda::Function helloFromLambdaFunction -UPDATE_COMPLETE AWS::Lambda::Function helloFromLambdaFunction -UPDATE_COMPLETE_CLEANUP_IN_PROGRESS AWS::CloudFormation::Stack sam-app -DELETE_IN_PROGRESS AWS::CloudFormation::Stack AwsSamAutoDependencyLayerNestedStac -DELETE_COMPLETE AWS::CloudFormation::Stack AwsSamAutoDependencyLayerNestedStac -UPDATE_COMPLETE AWS::CloudFormation::Stack sam-app ------------------------------------------------------------------------------------------------------------------------------------------------------Successfully created/updated stack - sam-app in eu-west-1Step 7: Clean upWe only have one stack in our AWS Account.To delete it, you can run the sam delete command which deletes the main stack (sam-app).Next steps If you need more information about SAM I recommend you to visit the AWS documentation here. Next post: How to add CI/CD to my SAM project Comment this postComment this post I have temporarily added the comments section to the post here. In the future, I will add it in a better way and include all the validated comments (I guess that I will have to make a filter to avoid spam) Submit Submitting... Failed to submit the form Please, try again. Thank you! I will add your comment in a future. " }, { "title": "How to add CI/CD to my CDK project", "url": "/posts/how-to-add-ci-cd-to-my-cdk-project/", "categories": "How-to, AWS-console, IaC", "tags": "cdk, cicd, codepipeline, codebuild", "date": "2022-03-26 02:03:00 +0100", "snippet": "TLDRI already have a CDK project on GitHub here, but to deploy it I have to run the CDK Toolkit command cdk deploy from my local machine.I want to add automation to my deployment process and integrate it with the AWS ecosystem… so I will use the AWS Developer tools to do it.I will show you 2 different approaches: Create the pipeline with the AWS Console: helps to understand how the services involved work Create the pipeline with IaC (CDK): best practice, always automate everything. So in this case yes, I also want to automate the creation of the application automation deployment!I want to implement the simplest solution, with the KISS principle in mind, and for this reason, my architecture diagram is as follows:Explanation: In a cdk deployment, I don’t need to run the cdk synth command and manage the generated artifacts, so the simplest solution is to run the cdk deploy command directly. If you need more information about it, I wrote a related post: How to create infrastructure with CDK.However, upon investigation the AWS recommendation to deploy a CI/CD pipeline of CDK projects is something similar to the following:I will explain it in detail in this post. I have preferred to include all of information in the same post, although I could easily split it in 2 or 3, and this post have a lot of content and images.IntroductionBefore I start showing you how to add the CI/CD I will introduce you to the basic concepts involved:SDLC (Software Development Lifecycle) is a process for planning, creating, testing, and deploying an information system - WikipediaDepending on where you look, there will be a different number of phases in the SDLC process.For this article we will explain what means CI/CD over 4 phases of the software release process: source, build, test and production (deployment):CI/CD refers to Continuous Integration and Continuous Delivery and it introduces automation and monitoring to the complete SDLC.Continuous integration (CI) is a software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run The key goals of CI are to find and address bugs more quickly, improve software quality, and reduce the time it takes to validate and release new software updates Continuous integration focuses on smaller commits and smaller code changes to integrateContinuous delivery (CD) is a software development practice where code changes are automatically built, tested, and prepared for production release Benefits Automate the software release process Improve developer productivity Improve code quality Deliver updates faster The point of continuous delivery is not to apply every change to production immediately but to ensure that every change is ready to go to productionContinuous deployment (CD), revisions are deployed to a production environment automatically without explicit approval from a developer, making the entire software release process automated. So yes, the “CD” in “CI/CD” means 2 different things: Continuos Delivery (prepare deployment to prod) and Continuos Deployment (deploy automatically in prod) CodePipeline for CDK with AWS ConsoleFirst, we will create the solution with the AWS Console because it helps to understand how the services involved work.As we want to create a new Pipeline, we must access to CodePipeline service and click on Create pipeline.Step 1 in CodePipeline is to choose the pipeline settings. We have to create a new service role (or use an existing one).Step 2 is to add the source of the stage choosing the source provider. I have my code repository on GitHub so I choose GitHub (version 2) but you could choose a different one. There are 2 options for GitHub source provider: version 1 (not recommended) which uses OAuth apps to access your GitHub repository version 2 (recommended) which uses a connection with GitHub Apps to access your repository If you choose GitHub version 2, the next step is to create a new connection to GitHub and if you don’t have any GitHub App created you need to create a new one, so you should click on Install new app. You have to choose whether to create the connection for all repositories or only th selected ones, and click Install. The GitHub connection is ready to use and you will be redirected to Step 2 of the creation of the CodePipeline.Now you can choose your repository and your branch and click on Next.Step 3 is to add the build stage, and you should select AWS CodeBuild because we want to use this service to add custom commands.After that, select the region, a project name, and a single build and click to Next. With this specific configuration, it will fail, do you know why?Step 4 is to add the deploy stage. Here are all the available options but we skip this step because we don’t need it.Now the CodePipeline is ready to be created and a review page is displayed. Confirm and create the CodePipeline.It is done. We have created the CodePipeline and added it 2 stages: Source BuildFirst execution…As you can see the execution had failed!Do you know what caused the error? Let’s investigate it…If you click on the execution ID link, You are redirected to the pipeline execution summary and you can see the error message Project cannot be found` in CodeBuild.Also, you can click on the AWS CodeBuild action name and you will be redirected to CodeBuild service… where you will receive the same error information: “Resource not available”. Yes, there is no CodeBuild project created in the pipeline, we just add a name of a created CodeBuild resource (and this resource doesn’t exist because nobody has created it).To fix it, you need to edit the Pipeline and edit the Build stage to create a new CodeBuild project.A new window will be opened, the Build stage will be editable and you need to click on the Create project button.You can create the build project by choosing the following: Operating System: Amazon Linux 2 Runtime(s): Standard Image: the more updated image New role name Buildspec: Insert build commands and click to Switch to editor and add the following: version: 0.2phases: install: commands: - npm install - npm install -g typescript - npm install -g aws-cdk build: commands: - npm ci - npm run build - cdk deploy Add a CloudWatch log, choosing as Group name /aws/codebuild/blog-infrastructureWhen you have finished filling in all fields, click Continue to CodePipeline. Again, with this configuration the execution will fail. Do you know why?You can now go back to the CodePipeline and force the execution again by clicking to Release change.And, as expected, it fails again.This time we will review the CloudWatch logs generated for this run to look for errors.You can see that the CodeBuild role is trying to assume the CDK role to perform the cdk commands, and of course, we didn’t specify any permissions to the new role so it can’t assume any roles. How CDK deploy works: Behind the scenes, when the cdk deploy command is executed, CDK is using the CDK roles created in the bootstrap process to perform some actions: perform a lookup, upload files and deploy the template uploaded in the S3 into the CloudFormation service.Therefore, you need to update the CodeBuild role to add the assumed permission to cdk roles. To do this, create new permission (new inline policy).You must to add the Action sts:AssumeRole and the Resources of the 4 CDK roles created in the bootstrap.When is created, you can review that the new permission has been added to the CodeBuild role.If you come back to the CodePipeline service and you execute it again, it will succeed! Now, if you make any changes in your repository, the pipeline will be automatically executed and your infrastructure will be updated executing the cdk deploy command of the CDK Toolkit inside of the CodeBuild service.If you want, you can check the logs in CloudWatch service to verify that the execution of the cdk deploy command went as we expected:Improve: Use a Buildspec file inside the codeYou have done a lot of manual work, and the first improvement you can automate is the definition of the build process itself.You need to update in the CodeBuild project the buildspec configuration of the project, choose Use a buildspec file and click to Update buildspec.Now when the pipeline runs it will look for the buildspec file inside the code (in the root folder). You have configured the CodeBuild service but you don’t have the buildspec.yml file added to your code yet.Next, you must to add the buildspec.yml file with the same content you provided in the online editor to update the build commands in the code. More information about buildspec fileIn the following image, you can see the VSCode IDE and the new buildspec.yml file with the same content as before.Test it: automatic execution of the pipeline when a commit is doneIf you commit the new file to your repository (buildspec.yml)The pipeline runs automatically as expectedCodePipeline for CDK with IaCNow that we have deployed the CodePipeline with the AWS Console, we will do the same with Infrastructure as Code with CDK.To do this, I will add a CodePipeline resource to my CDK project of the blog. I am using the GitHub v2 connection because it is the recommended way, and it requires first to use the AWS Console to authenticate to the source control provider, and then use the connection ARN in your pipeline definition. In other words, if you use GitHub v2 you need create the connection manually!Selfmutation property I want to show you first how “selfmutation” works in CDK pipelines because this it is important to know.This is the code to add the CodePipeline resource with 2 stages: Source with GitHub v2 (with a connection) Build phase (cdk synth) CDK pipelines will generate CodeBuild projects for each ShellStep you useconst codePipelineName = `blog-infrastructure-cdk`;const pipeline = new CodePipeline(this, codePipelineName, { pipelineName: codePipelineName, synth: new ShellStep('Synth', { // input: CodePipelineSource.gitHub('alazaroc/aws-cdk-pipeline', 'main'), input: CodePipelineSource.connection( 'alazaroc/blog-infrastructure', 'main', { connectionArn: 'arn:aws:codestar-connections:eu-west-1:xxxxxxxx:connection/fb936fb8-a047-43d5-90bd-xxxxxxxxxx', // Created using the AWS console * });', }, ), commands: ['npm ci', 'npm run build', 'npx cdk synth'], }),}); You must deploy the pipeline manually once. After that, the pipeline will be kept up to date from the source code repository.If you run the cdk deploy command, as you can see the pipeline is created and automatically runs. But wait a minute, we have three stages? A new one SelfMutate appears. Well, let’s wait to finish… PipelineNotFoundException? What happened here? We waited for the pipeline to finish executing and the pipeline no longer exists!Let’s do some research on SelfMutate. The CDK documentation says: Whether the pipeline will update itself This needs to be set to true to allow the pipeline to reconfigure itself when assets or stages are being added to it, and true is the recommended setting. You can temporarily set this to false while you are iterating on the pipeline itself and prefer to deploy changes using cdk deploy.We haven’t added this property to our code and the default value applied is true, so the pipeline has updated itself and, as the code is NOT committed in the source code, the pipeline has been removed.We have previously executed the deploy command but not a previous commit. I didn’t commit my CodePipeline code to make the result “more dramatic” (pipeline deleted automatically). But if you commit the code, nothing will happen. You will have one more stage to allow the pipeline to autoconfigure if there any changes, and at each run this will be checked.I want to show you what will happen if you set the selfmutate property to false (and the code is not committed).This is the change needed in the to the CDK CodePipeline resource code, adding this line:selfMutation: false,And if you run the cdk deploy command again, the pipeline will be updated:Now, there are only 2 stages in the CodePipeline, the Source and the Build, the 2 that we have configured and work perfectly.CDK DeployWe will change the code of our CodePipeline service so that instead of executing a cdk synth command, it will execute the cdk deploy command.Also, to avoid the assumed role error we saw in the AWS Console example (in this same post), we will add the IAM permissions necessary to the CodeDeploy role to run the deploy command. To customize the CodeBuild project, change ShellStep by CodeBuildStep. This class has more properties to customize it:const codePipelineName = `blog-infrastructure-cdk`; const pipeline = new CodePipeline(scope, codePipelineName, { pipelineName: codePipelineName, // synth: new ShellStep('Deploy', { synth: new CodeBuildStep('Deploy', { input: CodePipelineSource.connection( 'alazaroc/aws-cdk-pipeline', 'main', { connectionArn: 'arn:aws:codestar-connections:eu-west-1:xxxxxx:connection/4d6c1902-bda7-43fb-8508-xxxxxx', }, ), commands: ['npm ci', 'npm run build', 'npx cdk deploy --require-approval'], rolePolicyStatements: [ new aws_iam.PolicyStatement({ actions: ['sts:AssumeRole'], resources: ['*'], conditions: { StringEquals: { 'iam:ResourceTag/aws-cdk:bootstrap-role': [ 'lookup', 'image-publishing', 'file-publishing', 'deploy', ], }, }, }), ], }), selfMutation: false, });When you deploy it will create the CodePipeline project and execute the 2 steps defined: Source Build (cdk deploy) We changed the cdk synth by cdk deploy and also added the necessary permissions.And since we have added the appropriate permissions, it doesn’t fail.Recommended deployment of CodePipeline to CDK projects This approach is a little different.I will use this other example of CodePipeline for CDK, to make it easier to understand. Also, I will go step by step to understand perfectly how it works.This is the final diagram of what we will build (with the cdk code): If you want to create the Pipeline of the CDK project you will need to include at least two stacks: one for the pipeline and one or more for the infrastructure that will be deployed with the pipeline.Application deployment begins by defining MyPipelineAppStage, a subclass of Stage that contains the stacks that make up a single copy of my stack (MyIaCStack).export class MyPipelineAppStage extends Stage { constructor(scope: Construct, id: string, props?: StageProps) { super(scope, id, props); const iaCStack = new MyIaCStack(this, 'iac-example-stack', { description: 'Stack created with codepipeline in the example aws-cdk-pipeline', }); }}Now, we define MyIaCStack, which contains all the AWS resources that will be created in a different stack than Pipeline:export class MyIaCStack extends Stack { constructor(scope: Construct, id: string, props?: StackProps) { super(scope, id, props); // Add resources new aws_s3.Bucket(this, 'MyFirstBucket', { enforceSSL: false, }); }}Finally, we create the main stack, MyPipelineStack, which will add MyPipelineAppStage as a stage within the CodePipeline resource.export class MyPipelineStack extends Stack { constructor(scope: Construct, id: string, props?: StackProps) { super(scope, id, props); const codePipelineName = `test-iac-with-cdk`; const pipeline = new CodePipeline(this, codePipelineName, { pipelineName: codePipelineName, synth: new ShellStep('Synth', { // input: CodePipelineSource.gitHub('alazaroc/aws-cdk-pipeline', 'main'), input: CodePipelineSource.connection( 'alazaroc/aws-cdk-pipeline', 'main', { connectionArn: getMyGitHubConnectionFromSsmParameterStore(this), // Created using the AWS console * });', }, ), commands: ['npm ci', 'npm run build', 'npx cdk synth'], }), }); pipeline.addStage( new MyPipelineAppStage(this, 'Deploy', { // env: { account: \"111111111111\", region: \"eu-west-1\" } }), ); }}We have to commit all the above changes, and after that deploy it.It will create: the main stack with the CodePipeline, MyPipelineStack, the Deploy-iacStackFirst, the pipeline stack is created, and then, when the CodePipeline is executed, the second stack which contains all the other resources is created.That is all, we have automation in our deployment process!So as you can see, using CDK’s CodePipeline constructor, the following is created:Comment this post I have temporarily added the comments section to the post here. In the future, I will add it in a better way and include all the validated comments (I guess that I will have to make a filter to avoid spam) Submit Submitting... Failed to submit the form Please, try again. Thank you! I will add your comment in a future. " }, { "title": "How to create infrastructure with CDK", "url": "/posts/how-to-create-infrastructure-with-cdk/", "categories": "How-to, IaC", "tags": "cdk, cloudformation, github", "date": "2022-03-16 19:28:00 +0100", "snippet": "TLDRI will explain the basics of CDK in practice: how CDK works and how to deploy a CDK project from scratch.In addition, I will share the source code of my CDK project used to create the infrastructure of my blog.IntroductionThis section contains: What is CDK How CDK works PrerequisitesWhat is CDKCDK is an open-source software development framework to define your cloud application resources using familiar programming languages (TypeScript, JavaScript, Python, Java, C#/.Net, and Go)AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. To me, with a developer background, CloudFormation is complex and CDK fills the gap because it allows me to use a programming language to create the infrastructure easily, it’s wonderful.How CDK worksTo interact with CDK apps you will need the AWS CDK Toolkit (command-line tool).In a nutshell: Add code: Add the desired AWS resources in the app code with your preferred programming language. Transform the code into a CloudFormation template: Run the cdk synth command from the AWS CDK Toolkit to generate the CloudFormation template from the app code. Deploy the infrastructure: Run the cdk deploy command from the AWS CDK Toolkit to create a new stack on the CloudFormation service, which will deploy the AWS resources to the configured AWS account.AWS CDK Toolkit commands you need to know: Command: Function cdk init: Creates a new CDK project in the current directory from a specified template cdk bootstrap: Deploys the CDK Toolkit stack. It must be executed once per environment (account and region) to allow cdk to create the resources it needs to run cdk synthesize / cdk synth: Synthesizes and prints the CloudFormation template for the specified stack(s) cdk diff: Compares the specified local stack with the deployed stack cdk deploy: Deploys the specified stack(s) cdk destroy: Destroys the specified stack(s) Prerequisites AWS CLI how to install it how configure it Node.js official website IDE for your programming language VSCode Others AWS CDK Toolkit npm install -g aws-cdk cdk version cdk bootstrap You must execute it once per environment (account and region) to allow CDK to create the resources it needs to run How to create infrastructure with CDKThis section contains: How to create and deploy a basic CDK application Clean up Make changes to the default CDK application and deploy itHow to create and deploy a basic CDK applicationI have chosen TypeScript as my programming language.TypeScript is a strongly typed programming language that builds on JavaScript, giving you better tooling at any scale.The typescript sources need to be compiled into JavaScript.What is the fastest way to create a new CDK project and deploy it in your AWS account?# Create an empty foldermkdir cdk-basic-example &amp;&amp; cd cdk-basic-example# Creates an example of CDK Application with some constructscdk init sample-app --language typescript# Deploy and generate the resources into your AWS accountcdk deploy --require-approval neverAnd that’s all, we have deployed one topic and one queue in our AWS Account… NOTE: What about the generation of the CloudFormation template in the synth phase? We have executed only the deploy command… When you execute cdk deploy behind the scenes also… is executed cdk synth to generate the CloudFormation template (so you could want to avoid execute cdk synth before cdk deploy) and our assets code and the CloudFormation template are deployed to the S3 bucket provisioned when cdk bootstrap was executed Clean upLet’s destroy the stack. I know this section maybe should be at the end, but where’s the fun in that? We are playing and we need to try different things. If you try to do something different and on your own, you will learn faster!# Delete the CloudFormation stack (so it will delete all resources related)cdk destroy --forceYou have to know how to destroy a stack, so remember to do it at the end if you are playing with cdk…Make changes to the default CDK application and deploy itPerhaps we want to check what is to be deployed before we deploy it?It makes sense to me.# Synthesizes the CloudFormation template for the specified stack(s)# In the console, the template will be printed in yaml format# In the \"cdk.out\" folder, the template will be in json formatcdk synthWhat if we compare the local code with the stack deployed in the AWS account?cdk diff NOTE: cdk diff needs to connect to the AWS Account to check the CloudFormation stack against your local resources.We can see all the new resources that will be created: [+] AWS::SQS::Queue [+] AWS::SQS::QueuePolicy [+] AWS::SNS::Subscription [+] AWS::SNS::Topic Remember that we have destroyed the stack in the previous step so all resources are detected as new.We deploy it again:cdk deploy --require-approval neverAnd we run the diff command again to see the differences between the local code and the deployed stack:cdk diffNow le’s update the cdk code to generate some differences. First of all, we have to open the project with our IDE (you can also do it with a notepad but…)This file contains the 2 AWS resources of my example, a queue (red) and a topic (yellow). I could add a new service but for simplicity, I will remove the topic (lines 15 to 17) and run the cdk diff again.cdk diffWe can see that we have deleted the topic in the code, and when we run the diff command CDK finds the changes and shows them to us.And that’s all, keep practicing and learning! Remember to destroy your stack when you are done playingBonus: My CDK blog codeThe source code is available here.If you review it and think it can be improved, please let me know.Next steps If you need more information about CDK I recommend you to visit the AWS documentation here. Next post: How to add CI/CD to my CDK project Comment this postComment this post I have temporarily added the comments section to the post here. In the future, I will add it in a better way and include all the validated comments (I guess that I will have to make a filter to avoid spam) Submit Submitting... Failed to submit the form Please, try again. Thank you! I will add your comment in a future. " }, { "title": "How to deploy a web with Amplify Hosting", "url": "/posts/how-to-deploy-a-web-with-amplify-hosting/", "categories": "How-to, AWS-console", "tags": "amplify, route53, github", "date": "2022-03-15 21:13:00 +0100", "snippet": "TLDRThis is a practical use case where I will explain step by step how I deployed my blog using Amplify Hosting. I will use the AWS Console to do it.The source code of my blog (web) is available here.Introduction AWS Amplify is a set of purpose-built tools and features that enables frontend web and mobile developers to quickly and easily build full-stack applications on AWS. Amplify provides two services: Amplify Hosting and Amplify Studio. Amplify Hosting provides a git-based workflow for hosting full-stack serverless web apps with continuous deployment. Amplify Studio is a visual development environment that simplifies the creation of scalable, full-stack web and mobile apps. Use Studio to build your frontend UI with a set of ready-to-use UI components, create an app backend, and then connect the two together. AWS Amplify is the fastest and easiest way to develop and deploy reliable and scalable mobile/web applications on AWSHow to deploy a web with AmplifyWe need to have our code ready to be deployed in a supported repository. Supported repositories: GitHub, Bitbucket, GitLab and AWS CodeCommit. Another option is to deploy manually with drag and drop, Amazon S3 or any URL.In AWS Console, enter to AWS Amplify service and choose Amplify Hostinga) If you don’t have any Amplify resource, this screen appears and you have to click on Get Startedand then choose Amplify Hostingb) If you already have an Amplify resource click on New app and Host web appConfigure Amplify Hosting:The first step to set up Amplify Hosting is to connect your repository. In my case, I chose GitHub Add repository branch: choose your repository and your branch Configure build settings: Advanced settings are optional, allow you to reference your build image, add environment variables and override default installed packages Review the configuration and click Save and deploy Now Amplify must be provisioned, built, deployed and verified At this moment an email notification will be sent to your email (at least in the GitHub case) When finished, you will be able to access the new URL generated by Amplify How to associate my web with my domain nameWe can also set up our Domain Name to our deployed website with Amplify Hosting easily.To register the domain name I used Amazon Route53. Use Route53 is not the cheapest option (e.g. I paid $12 to register the new domain with Route53 instead of $1 the first year with GoDaddy), but it’s worth it (to me)In the side menu, click Domain management, and then click Add domain.Choose your Domain and click on Save. As I register my domain with Route53, it appears in the text field when I click on it.Now you can choose the branch, the subdomains and the check of automatic redirect from HTTP to HTTPS, and click Save.It may take several minutes to complete. First, you need to create the SSL certificate, then configure the SSL and finally activate it.If we access the main page we can see that the URL has changed.We can now access it with our domain name:That’s it, quick and easy!Comment this post I have temporarily added the comments section to the post here. In the future, I will add it in a better way and include all the validated comments (I guess that I will have to make a filter to avoid spam) Submit Submitting... Failed to submit the form Please, try again. Thank you! I will add your comment in a future. " }, { "title": "The technology behind this blog", "url": "/posts/the-technology-behind-this-blog/", "categories": "General", "tags": "amplify, cdk, github, serverless", "date": "2022-03-02 21:49:00 +0100", "snippet": "TLDRMy technology approach is: Use of AWS resources when possible. One of the reasons for creating this blog is to practice with AWS… and as a first step, the blog itself must use AWS resources. However, as a good rule, there is an exception GitHub is used as code repository because I want to share my code easily in a public way Serverless architecture Apply the separation of concerns design principle to frontend and backend: Frontend: Static website generated with Jekyll and deployed with AWS Amplify Code here Backend: AWS resources deployed with CDK using the TypeScript language Code here FrontendI have included the following in this section: Decision 1: Technology to create the blog Decision 2: Technology to deploy the blog Demo: How to deploy itI could add much more details but I want to keep it short.Technology to create the blogFirst of all, I needed to choose how to create the blog, and nowadays there are a lot of options to do it in a serverless way: Single Page Application (SPA): React, Angular, Vue.js, Ionic, Ember Server-Side Rendering (SSR): Express.js, Next.js, Nux.js, Gatsby.js Static Site Generator (SSG): Gatsby, Next.js, Nuxt.js, Hugo, Jekyll, Hexo Progressive Web Apps (PWA): React, Angular, Vue.js, Preact, PWABuilderSince I wanted to keep it simple, I used a static site generator. I must admit I hesitated with Hugo, Jekyll, and Hexo, all three options were good for me and although I liked Hugo for its fast build times and execution performance, I finally decided on Jekyl just because the theme I used (Chirpy) I liked more visually than the other options and I didn’t want to have to customize it too much. I am a big fan of the KISS design principle!Technology to deploy the blogAfter choosing Jekyll as my static site generator, I needed to know how to deploy it on AWS and, of course, there are many options to do it on AWS: EC2 + RDS (i.e. traditional blog with WordPress / Ghost + Gatsby / …) LightSail (by the way, an interesting article comparing LightSail with EC2) Container solutions (ECS/EKS) AWS ElasticBeanstalk S3 AWS AmplifyBut like I want it serverless and simple, I’ve leveraged AWS Amplify to help me with this point. What is AWS Amplify? (Explained by AWS) AWS Amplify is a set of purpose-built tools and features that enables frontend web and mobile developers to quickly and easily build full-stack applications on AWS. Amplify provides two services: Amplify Hosting and Amplify Studio. Amplify Hosting provides a git-based workflow for hosting full-stack serverless web apps with continuous deployment. Amplify Studio is a visual development environment that simplifies the creation of scalable, full-stack web and mobile apps. Use Studio to build your frontend UI with a set of ready-to-use UI components, create an app backend, and then connect the two together. AWS Amplify is the fastest and easiest way to develop and deploy reliable and scalable mobile/web applications on AWSI used Amplify Hosting for the following reasons (there are more, but the following are important to me): Serverless (uses S3 and CloudFront behind the scenes) Integrates with my existing code in GitHub Supports Jekyll (my static site generator) Manage the CI/CD of my application Easy way to connect my application with my custom domain Instant cache invalidations in new versions Integrated with Amazon CloudWatchAs you can see, this solution is awesome if you want that AWS manage for you the CI/CD, web, cache, certificate of your domain… However, this is too “automagic” for me and I am here to practice/play and show you the results… so in the future I’d like to migrate Amplify to a custom solution to have more control and more services to play with (S3, CloudFront, AWS Certificate Manager, Developer Tools)… a lot of fun is waiting for me!How to deploy itI wrote it in this post: How to deploy a web with amplify hostingAnd I complemented it with this one: How to add CI/CD to my CDK projectBackendI have included the following in this section: Decision 1: What resources to create Decision 2: Technology to deploy infrastructure Demo: How to deploy infrastructureWhat resources to createNo backend is necessary for the blog to work. Simple blogs that only have content don’t need anything more than static pages. However, if you want more functionality like forms, mail subscriptions, or comments you will need to use external plugins (to store the data somewhere else, not on AWS) or create your own solutions.I don’t want to use external plugins if I can do it “just the same” myself in AWS and practice/play with new services in the processAfter creating my empty blog I thought that it would be a good idea to implement the following: Forms (contact form) Mail Subscription (to send updates of the blog) Add comments to each post (register it and show it)Now, I have a basic implementation of these points but I will improve it in the future.This is the architecture diagram of the AWS resources created:Backend Architecture Diagram The backend can also be integrated with the frontend with Amplify Studio, but I am not interested in doing it that way. I want separation of concerns and manage both independently.Forms Used here: Contact form External option easy to integrate: Google Forms. Custom AWS solution: flowchart LR A(Contact form) --&gt; B(API Gateway) B --&gt; C(Lambda) C --&gt; D(SES) D --&gt; E(My email) Mail Subscription Used here: Mail subscription External option easy to integrate: Mailchimp Custom AWS solution: flowchart LR A(Mail subscription form) --&gt; B(API Gateway) B --&gt; C(Lambda) C --&gt; D(DynamoDB) At this moment, I only store the subscription information and if I want to send emails I have to do it manually. However, in the future, I will automate it and I will add the option to unsubscribe (to the sent mail)Comments Used here: Comment External option easy to integrate: Disqus Custom AWS solution flowchart LR A(Mail subscription form) --&gt; B(API Gateway) B --&gt; C(Lambda) C --&gt; D(DynamoDB) At this moment I only store the comments of each post in a DynamoDB table and in the future I will show all validated comments in the postsTechnology to deploy infrastructureHonestly, I did not evaluate other options, since I knew which one to choose.I use CDK (Cloud Development Kit) with TypeScript programming language to create the backend services. What is CDK? (Explained by AWS) CDK is an open-source software development framework to define your cloud application resources using familiar programming languages. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation, but also is available (in alpha phase) a CDK for Terraform cdktf and a CDK for Kubernetes cdk8s. To find all of these CDKs in one place, check out Construct Hub, a place to discover and share construct libraries published by the open-source community, AWS, and partners. To me, with a developer background, CloudFormation is complex and CDK fills the gap because it allows me to use a programming language to create the infrastructure easily, it’s wonderful.How to deploy infrastructureI wrote it in another post: How to create infrastructure with CDKPrice estimation of the blogI just created the AWS Account, so I will use the free tier.Price information by services used: Route53 AWS Amplify API Gateway DynamoDB Lambda I will update it when my free tier expires AWS resource Action Free Tier (per month) Estimation price Route53 Domain Registration $1 for domain $1 Route53 Hosted Zone $0.50 per Hosted Zone for the first 25 $0.50 AWS Amplify Build &amp; Deploy $0 for 1000 build minutes $0 AWS Amplify Hosting 5 GB stored $0 AWS Amplify Hosting $0 for 15 GB served $0 API Gateway API calls for REST API $0 for 1 million $0 DynamoDB On-demand Data Storage on Standard table $0 for 25 GB $0 DynamoDB On-demand Data transfer out to the internet $0 for 100 GB $0 Lambda Requests per month $0 for 1 million $0 Lambda Compute time $0 for 400,000 GB-seconds $0 CDK &amp; CloudFormation Free to use   $0 TOTAL 1.5$ per month. Taxes are NOT included. The domain name purchase on Route53 is annual and is paid in the month of purchase, but for simplicity, I split it into each month. Also, the invoice category is NOT Route53 but “Registrar”, “Global Region”, and “Amazon Registrar DomainRegistration”. Use Route53 for Register Domain is not the cheapest option. I paid $12 instead of around $1 for the first year with GoDaddy, but it’s worth it (to me)Next steps about blog technologyI have many next steps identified, but I’ll put here the ones related to the content of this post. Update comments form Show comments in the posts Automate Mail subscription Migrate AWS Amplify Web to S3 + CloudFront + AWS Certificate Manager + Developer ToolsComment this post I have temporarily added the comments section to the post here. In the future, I will add it in a better way and include all the validated comments (I guess that I will have to make a filter to avoid spam) Submit Submitting... Failed to submit the form Please, try again. Thank you! I will add your comment in a future. " } ]
